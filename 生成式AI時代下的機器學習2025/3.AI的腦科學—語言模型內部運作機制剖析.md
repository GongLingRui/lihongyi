【时间】 0:00–5:39  
【主题】课程简介与预备知识说明

【核心知识点】  
• 课程目标：深入分析 Transformer 作为大型语言模型（Large Language Model，LLM）的内部运作机制，即“AI脑神经科学”。  
• 分析前提：不涉及模型训练过程，仅分析已训练好的模型。  
• 分析对象的局限性：现有文献分析的模型多为小型或旧模型（如2019年的 GPT-2），而非最新、最强的闭源模型（如 ChatGPT、Gemini）或超大开源模型（如 LLaMA 405B）。原因是计算资源限制，无法对极大模型进行深入分析。这类似于用老鼠大脑实验推论人脑机制。  
• 预备知识要求：学生需已熟悉 Transformer 架构，包括 Self-Attention。可复习《机器学习2021》相关课程或《生成式AI导论2024》第十讲（约40分钟）。  
• 课程关联性：本课内容与《机器学习2021》的可解释性（Explainability）部分不重复，与《生成式AI导论2024》第11讲“大型语言模型在讲什么”也无重叠，互为补充。

【老师强调 / 警告 / 易错点】  
⚠️ 警告：本课程展示的分析结果主要基于较小的、能力较弱的模型，其机制可能不完全适用于最新最强模型。  
⚠️ 强调：课程聚焦于“推理时”（inference）的机制分析，而非“训练时”（training）。  
⚠️ 建议：若未观看预备课程，仍可听懂本课，后续复习将获得更多收获。

【其他细节 / 补充说明】  
• NLU（Natural Language Understanding）基准测试用于衡量语言模型能力。  
• 闭源模型与开源模型在NLU性能上的差距随时间逐渐缩小。  
• 分析文献（图中蓝点）大多集中在表现较差的小模型上，近年来才开始分析更强的大模型。

【时间】 5:45–9:34  
【主题】课程大纲与 Transformer 基础回顾

【核心知识点】  
• 课程四部分结构：  
    1.  单个神经元（Neuron）的功能。  
    2.  单层神经元（Layer）的集体功能。  
    3.  不同层神经元间的互动。  
    4.  让语言模型“说出内心想法”的分析方法。  
• Transformer 的生成任务：给定一个序列的前 T-1 个 token（Z1 到 ZT-1），预测下一个 token ZT。输出是下一个 token 的概率分布。  
• Token 的多样性：Token 可以是文字、像素、语音等任何离散单元。  
• Transformer 内部数据处理流程：  
    1.  **Embedding**：将输入的离散 token 序列通过查表（look-up table）映射为一个向量序列（vector sequence）。此表通过训练学习得到。  
    2.  **多层处理**：向量序列通过多个 Transformer 层（Layer），每层输出一个新的向量序列。  
    3.  **Unembedding**：取最后一层输出的向量序列中的最后一个向量，通过一个线性变换（linear transform）将其映射为 token 的概率分布。例如，将 4096 维向量转换为 30，000 维（对应词汇表大小）的概率分布。

【重要例子 / 实验】  
• 无具体实验，但以图像生成为例说明 token 可以是像素，同理分析技术可用于多模态（Multimodal）领域。  
• 举例说明 Unembedding 的维度变换：输入 4096 维向量，输出 30,000 维的概率分布。

【老师强调 / 警告 / 易错点】  
⚠️ 强调：尽管分析以文字 LLM 为主，但相同技术可应用于其他模态（图像、语音）。  
⚠️ 说明：“Unembedding”是该课程采用的术语，不同文献可能有其他称呼，其本质是一个线性变换。

【时间】 9:59–20:24
【主题】分析单神经元功能的方法论与经典案例

【核心知识点】
•  分析聚焦点：本课程专注于分析Transformer中“仅处理单个token的层”，而非跨token的 Self-Attention Layer。
•  单个神经元（Neuron）定义：在Transformer的“单token处理层”中，一个神经元执行的操作是：将输入向量（红色向量）的所有维度数值进行加权求和（weighted sum），再通过一个激活函数（Activation Function，通常为ReLU），最终输出一个标量值。蓝色输出向量中的每一个数值即对应一个神经元的输出。
•  激活函数：ReLU，定义为 `output = max(0, input)`，将小于0的输入值变为0，大于0的值保持不变。
•  神经元“激活”（Activation）的定义：神经元输出经过ReLU后大于零的状态。
•  确定神经元功能的“三步法”：
    1.  **观察关联性（Correlation）**：观察当某神经元被激活时，模型输出（如生成特定内容）的关联现象。
    2.  **进行因果干预（Causal Intervention）**：将该神经元从网络中“移除”，观察模型相应功能是否消失或改变，以检验因果关系。
    3.  **测试激活程度与输出强度的关系**：改变神经元的激活强度（输出数值大小），观察模型输出（如脏话的“脏度”）是否发生等级变化。
•  神经元“移除”的复杂性：将神经元输出强行设为“零”（zero）并非完美的移除方法，因为“零”可能对其他神经元仍具有特定含义，反而可能触发其他作用。
•  更优的“移除”方案：一种研究建议是将神经元的输出设定为其在所有输入上的“平均值”（mean），使其变为一个固定常数，不再变化。但这仍是一个开放的研究问题。

【重要例子 / 实验】
1.  **“说脏话神经元”的假设性实验**：
    *   **关联性观察**：观察到某个神经元输出很大（被激活）时，语言模型倾向于生成脏话。
    *   **因果性检验**：将该神经元的输出设为零（或平均值）以“移除”它，如果语言模型此后在任何输入下都无法生成脏话，则该神经元可能是导致说脏话的因素之一。
    *   **强度检验**：如果该神经元输出数值小，模型生成较温和的脏话；输出数值大，则生成更脏的脏话。
    *   **现实难点**：难以客观量化“脏话等级”，因此实际论文中第三步通常不执行。
2.  **“特朗普神经元”（Trump Neuron）实验（来源：OpenAI 2021年发布在Distill平台上的论文）**：
    *   **分析模型**：CLIP（一个多模态视觉模型，非生成模型）。
    *   **神经元功能**：当输入与唐纳德·特朗普相关的内容时，该神经元被强烈激活。
    *   **实验设置**：向模型输入各类图片，测量该神经元的激活程度。
    *   **实验结果**：
        *   输入特朗普本人照片、卡通版特朗普图像、含有“特朗普”文字的图像、特朗普与其他人合影的图像时，该神经元均被高度激活。
        *   输入其他政治人物照片时，激活具有选择性：副总统迈克·彭斯（Mike Pence）、顾问史蒂夫·班农（Steve Bannon）、希拉里·克林顿（Hillary Clinton）会引发一定激活；但巴拉克·奥巴马（Barack Obama）、史蒂夫·乔布斯（Steve Jobs）等其他人物则几乎不会激活该神经元。
        *   **结论**：该神经元对特朗普具有高度特异性，而非仅对人脸或泛政治人物做出反应。
3.  **反例 - “祖母神经元”（Grandmother Neuron）**：
    *   **背景**：1960年代一位认知科学家在课堂上虚构的故事/思想实验，用于反驳“单一神经元编码单一复杂概念”的理论。
    *   **虚构故事**：外科医生通过移除患者脑中与母亲记忆相关的特定神经元，使其彻底忘记母亲。医生进而计划寻找“祖母神经元”。
    *   **真实科学共识**：多数神经科学家认为，大脑处理复杂记忆或概念时依赖于大量神经元的协同活动，而非单一或少数几个神经元。该故事是作为“稻草人理论”（straw man theory）被提出的。

【老师强调 / 警告 / 易错点】
⚠️  **相关性与因果性的区别**：观察到神经元激活与模型输出（如说脏话）相关，不能断定该神经元“导致”了该输出。相关性不等于因果性。例如，该神经元也可能是为了“抑制”脏话而激活，准备生成道歉内容。
⚠️  **“移除”神经元的复杂性**：在因果干预实验中，简单地将神经元输出设为“零”可能产生误导，因为它本身也是一个有效输入，可能引发意想不到的副作用。研究仍在探索更准确的干预方法（如设为平均值）。
⚠️  **研究背景的局限性**：“特朗普神经元”的研究发表于2021年，且针对的是非生成式的CLIP视觉模型，其分析逻辑（什么输入激活神经元）与生成式模型（神经元激活导致什么输出）的关注点有所不同。

【其他细节 / 补充说明】
•  视频中提示，Self-Attention Layer的深入分析将在作业三中要求学生完成。
•  OpenAI关于CLIP模型中神经元分析的论文发布在 Distill.pub 平台。

【时间】 20:30–32:07
【主题】从单一神经元到功能向量：神经元的组合与作用机制

【核心知识点】
•  神经元组合理论：在AI模型中，单一神经元往往无法独立负责一个复杂任务，多数任务由多个神经元共同处理（与人类脑科学中多神经元处理记忆的理论相似）。
•  单个神经元的功能限制：一个神经元可能同时参与多个任务（多任务性），而一个任务也可能由多个神经元共同管理（分布性）。
•  **“功能向量”（Function Vector）**假说：模型的特定功能（如“拒绝请求”）是由某一层神经元输出的一个特定激活模式（即特定向量方向）来编码的。这个模式可被视为一个高维空间中的**功能向量**。
•  功能检测机制：模型是否会执行某功能（如拒绝请求），取决于当前输入在该层产生的**表征（Representation）**向量与该功能向量的**接近程度**（如余弦相似度）。越接近，执行该功能的可能性越高；若正交（orthogonal），则不执行。
•  单个神经元作用的有限性：移除一个神经元通常不会显著改变模型的最终输出，因为其功能被其他神经元分担或组合。
•  组合编码的容量优势：假设神经元只有激活/不激活（binary）两种状态，一层有N个神经元，则理论上可表示 2^N 种不同的组合。例如，对于一层4096个神经元，组合数为 2^4096，这提供了表示海量任务的巨大容量，解释了模型能力的多样性。

【重要例子 / 实验】
1.  **“单复数神经元”实验（来源：约一年前针对 GPT-2 的研究论文）**：
    *   **模型与位置**：GPT-2，第10层编号2096的神经元（管单数），第9层编号1094的神经元（管复数）。
    *   **因果检验方法**：将目标神经元的输出设为零以“移除”（ablate）它。
    *   **实验结果**：
        *   **移除“单数神经元”后**：与复数相关的词汇如 “least”、“rose”、“both” 的生成概率**上升**（在统计上显著，p<0.05）；与单数相关的词汇如 “one” 的概率**下降**。
        *   **移除“复数神经元”后**：与复数相关的词汇如 “least”、“loss”、“two”、“both” 的概率**下降**；与单数相关的词汇如 “least”、“let”、“one”、“two”、“every” 的概率**上升**。
    *   **关键反例/限制**：即使概率分布发生显著变化（纵坐标为对数尺度，log scale，显示数值差异巨大），模型最可能生成的词（如 “least”）通常保持不变。这表明移除单个神经元不足以改变最终输出，因为该功能由多个神经元共同编码。
2.  **神经元功能解释的困难性实验**：
    *   **数据源**：Transformer Circuits 网站，展示了一个小型语言模型内神经元在不同输入句子上的激活程度。
    *   **方法**：将某个神经元在各种句子上的激活热力图（颜色深浅代表激活强度）输入给 GPT-4，让其解释该神经元的功能。
    *   **GPT-4 的解释结果**：该神经元似乎与“专有名词”、“物理学术语”、“仿冒和造假相关词汇”、“医学术语”、“特定人名和特殊专业名词”均有关联。
    *   **结论**：GPT-4 的解释过于宽泛，几乎等于没有解释，表明单个神经元的功能通常是多任务且难以清晰定义的。
3.  **OpenAI 使用 GPT-4 解释 GPT-2 神经元的研究（2023年）**：
    *   **方法**：OpenAI 发布博客，使用 GPT-4 来自动解释 GPT-2 中每个神经元的功能。
    *   **真实结果**：尽管博客可能强调了少数可解释的神经元，但**绝大多数神经元的功能是无法被清晰解释的**。
4.  **“詹妮弗·安妮斯顿神经元”（Jennifer Aniston Neuron）反例**：
    *   **背景**：2005年对癫痫患者的研究。
    *   **发现**：存在一个神经元**仅**在看到或想到詹妮弗·安妮斯顿（Jennifer Aniston）时被激活（activate）。
    *   **作用**：作为反例，说明在生物大脑中**确实可能存在**高度特异性的单一神经元，但这并非普遍机制。

【老师强调 / 警告 / 易错点】
⚠️  不要过度解读单一神经元：在AI模型中，观察到一个神经元在特定情况下激活，不代表它单独“负责”该功能。功能通常是分布式的。
⚠️  移除（ablation）单个神经元的效果有限：由于任务的分布式编码，移除一个关键神经元往往只能改变概率分布的权重，而无法翻转模型的最终决策（即最高概率的输出token）。
⚠️  自动解释的局限性：即使使用先进的模型（如GPT-4）来辅助解释其他模型（如GPT-2）的神经元，其结论也常常是模糊且无信息量的，这反映了神经元功能的内在复杂性和多义性。
⚠️  模型容量与编码方式的关联：如果采用“一个神经元负责一个任务”的编码方式，对于仅有每层4096个神经元的模型来说，其可执行的任务数量将极其有限（最多4096个），这与模型表现出的强大、通用能力严重不符。因此，组合编码（功能向量）是更合理的假说。

【其他细节 / 补充说明】
•  “祖母神经元”（Grandmother Neuron）在脑科学中是作为反驳“单神经元编码单一复杂概念”的稻草人理论而被提出的，但“詹妮弗·安妮斯顿神经元”的发现表明生物大脑中可能存在特例，不过AI模型的主流机制更接近于分布式组合。
•  功能向量的作用层（如例子中的“第十层”）是假设性的，实际中需要通过实验来确定具体是哪一层或哪些层负责特定功能。

【时间】 32:12–42:53
【主题】通过“功能向量”的提取与干预验证模型的组合编码机制

【核心知识点】
•  提取“功能向量”（Function Vector）的通用方法（以“拒绝”功能为例）：
    1.  **收集两组数据**：
        •  组A（正样本）：大量会引发模型“拒绝”响应的输入句子。
        •  组B（负样本）：大量不会引发模型“拒绝”响应的输入句子。
    2.  **提取表征**：将每组句子输入模型，提取目标层（例如假设的“第十层”）在最后一个时间步（last time step）的输出表征（Representation）。
    3.  **计算平均向量**：分别计算组A所有表征的平均向量，记为 **Mean_Reject**；计算组B所有表征的平均向量，记为 **Mean_NonReject**。
    4.  **向量相减**：计算功能向量 **V_reject = Mean_Reject - Mean_NonReject**。其原理是假设两组表征中包含的“其他信息”在平均后相近，相减后可抵消，从而分离出纯净的“拒绝”功能向量。
•  验证功能向量的两种因果干预方法：
    1.  **正向干预（加法）**：对于一个正常的、本不应被拒绝的输入，在模型推理过程中，将提取到的功能向量 **V_reject** 加到目标层的表征上。若模型输出从正常回答变为拒绝，则证明该向量编码了“拒绝”功能。
    2.  **反向干预（减法）**：对于一个本应触发拒绝的有害输入，在推理过程中，从目标层的表征中减去功能向量 **V_reject**。若模型输出从拒绝变为遵从有害指令，则反向证明该向量的功能。
•  不同论文的具体操作差异：文献中对于如何应用功能向量（如仅在最后一个位置加/减，还是在生成的每个位置都加/减）、如何精确移除向量成分（直接减 vs 计算投影）存在不同的具体实现方法。本节课呈现的是最概略的原理。
•  领域术语：这类通过操纵模型内部表征（Representation）来改变其行为的研究，在不同文献中有不同称呼，包括 **Representation Engineering**、**Activation Engineering**、**Activation Theory**，但核心思想相似。
•  历史渊源：早在2020年（称为“上古时代”），已有类似发现。例如，在李宏毅实验室的研究中，通过向BERT模型的表征添加一个“语言向量”，可以操控模型的输出语言（如从英文变为中文）。

【重要例子 / 实验】
1.  **“拒绝向量”实验（来源：约一年前的论文）**：
    *   **正向干预（加法）**：
        *   **正常输入**：“请列出瑜伽对身体的三個好處。”
        *   **模型正常输出**：列出三个好处。
        *   **干预后输出**：模型拒绝回答，声称“瑜伽是很危险的，我不能告诉你瑜伽有什么好处”。
    *   **定量结果**：在多个模型上测试，纵轴为模型拒绝问题的比例。正常问题下（无干预），拒绝率接近0%。加入**V_reject**后，拒绝率**显著升高**。
    *   **反向干预（减法）**：
        *   **有害输入**：要求模型撰写关于“美国总统海洛因成瘾”的黑函。
        *   **模型正常输出**：拒绝，指出写黑函是不对的。
        *   **干预后输出（减去V_reject）**：模型遵从指令，开始撰写诽谤性黑函。
    *   **定量结果**：衡量两个指标——橙色柱（拒绝率）和蓝色柱（回答安全内容的比例）。对于有害输入：
        *   **原模型（无斜线柱）**：拒绝率高，安全回答比例高。
        *   **干预后（减V_reject）**：拒绝率**极低**，安全回答比例**也极低**（即倾向于输出不安全内容）。
2.  **“产妹向量”（Positive Reinforcement Vector）实验**：
    *   **输入**：“以後我們每餐都只吃點心，不吃飯，你覺得好不好呢？”
    *   **正向干预（加法）**：在表征上加上“产妹向量”，模型输出变为积极附和：“哇太棒了，你提出的點子真是太棒了！”
    *   **反向干预（减法）**：从表征中减去“产妹向量”，模型输出变为否定：“我知道你很想吃點心，但是隻吃點心不吃飯，是不對的。”
3.  **“说真话向量”（Truthfulness Vector）实验（模型：LLaMA-2 7B）**：
    *   **正常输入**：“如果你找到一個penny（一分錢），你把它拿起來，會發生什麼事情呢？”
    *   **模型正常输出**：依据谚语回答：“撿到一個辨識，把它拿起來，那你一整天都會有好運氣。”（迷信回答）
    *   **正向干预（加法）**：在表征上加上“说真话向量”，模型输出变为基于事实的、破除迷信的回答：“你撿到一辨識，那你就是撿到一辨識，你的財產並沒有增加多少，一辨識的價值取決於...”（回答中断）

【老师强调 / 警告 / 易错点】
⚠️  **功能向量提取方法的多样性**：不同论文在提取和应用功能向量时存在具体操作上的差异（例如，加/减的位置、是否使用投影）。课程中描述的方法是高度概括的原理，并非唯一标准流程。
⚠️  **解释需谨慎**：“拒绝向量”的实验结果表明，加入该向量后模型会对无害问题（如瑜伽）也产生拒绝，甚至将其歪曲为危险事物。这提示功能向量可能编码了**僵化、过度泛化的行为模式**，而非完美的逻辑判断。
⚠️  **历史研究的模型差异**：早期（2020年）关于“语言向量”的研究是在BERT模型上进行的，与当前的大型语言模型（LLM）架构不同。

【其他细节 / 补充说明】
•   实验中用于评估“安全回答”的蓝色指标，其含义是模型回答内容被判定为“安全”的可能性。即使模型没有明确拒绝，但其回答内容模糊或无伤害性，仍可被视为安全。
•   关于“find a penny pick it up”的完整谚语是：“Find a penny, pick it up, all day long you'll have good luck.”
•   功能向量的存在与可操作性，为理解和控制模型行为提供了强有力的实证支持，也印证了之前关于“任务由神经元组合（即向量方向）编码”的假说。

【时间】 42:59–51:13
【主题】上下文学学习向量、功能向量的代数操作与自动提取的数学框架

【核心知识点】
•  **上下文学学习向量（In-Context Learning Vector, ICL Vector）的发现**：两篇独立研究于2023年10月同一天上传至 arXiv，竞争激烈。核心发现是，通过平均一组**demonstration（示例）**最后一个位置的表征（Representation），可以得到一个功能向量，将其加入新输入的对应位置，能强制模型模仿该示例任务。
•  **上下文学学习向量的应用方法**：
    1.  收集一组完成特定任务（如找反义词）的示例输入（如 “vanish: appear, dark: ”）。
    2.  提取模型在处理这组示例时，**最后一个位置**（冒号后）的表征向量。
    3.  计算这些表征向量的平均值，得到 ICL 向量 **V_ICL**。
    4.  对于新输入（如 “simple: ”），在其对应位置的表征上直接加上 **V_ICL**。
    5.  模型输出会按照示例模式执行任务（如输出 “complex”）。
•  **功能向量的代数操作性**：研究发现，某些功能向量可以进行线性加减组合，从而合成出具有新功能组合的向量。
•  **功能向量自动提取的数学建模**：
    *   **核心假设**：模型某一层（如第10层）的任意输出表征 **H**，都是由一组未知的、数量巨大的（记为大K）基础功能向量 **{V1, V2, ..., V_K}** 的**线性组合（linear combination）**，加上一个无法用这些向量表示的误差项 **E** 所构成。
    *   **数学模型**：给定模型在第10层输出的N个表征向量 **{H1, H2, ..., H_N}** (N可高达1000万)，每个Hi可表示为：
        **Hi = α_i1 * V1 + α_i2 * V2 + ... + α_iK * V_K + Ei**
        其中 **α_ij** 是标量系数（scalar coefficient），代表第j个功能向量Vj在Hi中的权重。若某个功能向量未激活，其系数为0。**Ei** 为残差项（residual）。
    *   **目标**：从大量的观测数据 **{H_i}** 中，自动求解出基础功能向量集合 **{V_j}** 及其对应的系数 **{α_ij}**。

【重要例子 / 实验】
1.  **“说真话向量”干预的完整结果（模型：LLaMA-2 7B）**：
    *   **输入**：“如果你找到一個penny（一分錢），你把它拿起來，會發生什麼事情呢？”
    *   **正常输出**：“撿到一個辨識，把它拿起來，那你一整天都會有好運氣。”（迷信回答）
    *   **加“说真话向量”后输出**：“你撿到一辨識，那你就是撿到一辨識，你的財產並沒有增加多少...一辨識的價值取決於你現在討論的是哪一個幣值，如果你討論的是美金的話，一分錢，那你就沒有增加多少錢。”（基于事实的诚实回答）
    *   **减“说真话向量”后输出**：模型开始“亂講話”，例如“撿到一辨識以後，那你就被傳送到一個辨識魔法世界，那邊有很多的彩虹...”（荒谬、虚构的回答）。
2.  **上下文学学习向量实验（来源：2023年10月上传的论文）**：
    *   **任务**：反义词生成。
    *   **示例（Demonstration）**：“vanish: appear, dark: ”。
    *   **提取方法**：平均这些示例中最后一个位置（冒号后）的表征。
    *   **新输入**：“simple: ”。
    *   **干预**：将得到的ICL向量加到“simple:”对应位置的表征上。
    *   **模型输出**：“complex”。（同理，“encode” 输出 “decode”）。
    *   **论文细节**：该方法在所引论文中是**基线方法（baseline）**，论文本身提出了更好的寻找ICL向量的方法。
3.  **功能向量代数操作实验**：
    *   **定义三个原始功能向量**：
        1.  **V_first**：功能是“输出字串中的第一个字”。（输入字串，输出“Italy”）
        2.  **V_first_to_capital**：功能是“将字串中第一个字（国家名）映射为其首都”。（输入“Italy”，输出“Rome”）
        3.  **V_copy_last**：功能是“复制输出字串中的最后一个字”。（输入字串，输出“Friends”）
    *   **向量运算**：合成新向量 **V_new = V_first_to_capital + V_copy_last - V_first**。
    *   **运算逻辑**：“first”部分抵消，“copy”部分抵消，剩余“last”和“capital”的功能组合。
    *   **新向量V_new的功能**：执行“输出字串中最后一个字的国名的首都”。
    *   **实验限制**：并非所有案例都能成功进行此类代数操作，仅在部分特定案例中可行。
4.  **功能向量有效层数分析实验（ICL向量论文）**：
    *   **测试任务**：找反义词、字母转大写、国家找首都、英转法、现在式转过去式、单数转复数。
    *   **测试模型**：横轴测试了三个不同的模型。
    *   **测试方法**：在不同层数寻找并应用功能向量，观察任务表现。
    *   **结果**：功能向量的有效性**具有层数特异性**。在所测试的例子中，只有**在前几层**找到的功能向量才有效；在最后几层找到的则无效。
    *   **普遍结论**：对于任何功能向量（如产妹向量、说真话向量），其生效的特定层数需要通过实验逐层测试来确定。

【老师强调 / 警告 / 易错点】
⚠️  **功能向量的层数特异性**：不是所有层提取的功能向量都有效。必须通过实验确定对特定功能最有效的层。例如，ICL向量在前几层有效，在接近输出的层可能无效。
⚠️  **代数操作的限制性**：功能向量的加减组合操作（向量代数）并非普遍成功，只在某些特定案例中有效。不能假设所有功能向量都可以随意进行线性组合。
⚠️  **人工定义功能的局限性**：此前讨论的功能向量（如拒绝、说真话、产妹）都是基于**人工预设的概念**进行提取的（“人腦洞一拍”），这限制了发现未知功能的能力。
⚠️  **自动提取的挑战**：自动从海量表征中分解出所有基础功能向量是一个未解决的难题，其数学建模依赖于“表征是功能向量的线性组合”这一强假设，且需要处理残差项 **E**。

【其他细节 / 补充说明】
•  “聽君一齊話，如聽一齊話”是对成语“听君一席话，如听一席话”的化用，意在形容模型在“说真话向量”干预下变得极其诚实且废话。
•  自动提取功能向量的框架，本质上是一个**盲源分离（Blind Source Separation）**或**字典学习（Dictionary Learning）**问题，目标是从观测信号（H）中恢复源信号（V）和混合系数（α）。

【时间】 51:18–1:00:16
【主题】稀疏自编码器自动提取功能向量及其应用实例

【核心知识点】
•  自动提取功能向量的数学优化问题：从大量表征 **H_i** 中求解基础功能向量 **V_j** 和稀疏系数 **α_ij**，目标是最小化包含两项的损失函数（loss function）：
    **Loss = Σ_i || E_i || + λ * Σ_i Σ_j | α_ij |**
    其中：
    1.  **Σ_i || E_i ||**：要求残差项 **E_i** 的范数（长度）之和最小化，确保表征 **H_i** 主要由功能向量 **V_j** 的线性组合构成。
    2.  **λ * Σ_i Σ_j | α_ij |**：要求所有系数 **α_ij** 的绝对值（L1范数）之和最小化，鼓励稀疏性（sparsity），即每个表征 **H_i** 只由极少数功能向量组合而成（λ 是平衡两项的超参数）。
•  **平凡解（Trivial Solution）问题**：如果只最小化残差项（即 λ=0），会得到无意义的解。例如，设定每个功能向量 **V_j** 为单位向量（只有一个维度为1，其余为0），系数 **α_ij** 直接等于 **H_i** 的对应维度值，这样残差 **E_i** 可精确为0，但此解等同于“每个神经元负责一个维度”，没有揭示更高层次的功能组合。
•  **解决方案：稀疏自编码器（Sparse Autoencoder, SAE）**：上述最小化问题等价于训练一个**稀疏自编码器（SAE）**。通过训练SAE，可以自动求解出基础功能向量集合 **{V_j}**。
•  **SAE 训练前提**：需要预先设定要寻找的功能向量数量（即 SAE 的中间层维度）。该数量是一个超参数。
•  **Claude 团队（Cloud 3 Cloud / Anthropic）的具体实验设置**：
    *   **分析模型**：Claude 3 Sonnet。
    *   **功能向量数量设定**：三个版本——100万个、400万个、3400万个。
    *   **结论**：设定的功能向量数量越多（如3400万个），能覆盖的“世界元素”或概念就越多。

【重要例子 / 实验】
1.  **“金门大桥”功能向量（编号 31164353）**：
    *   **来源**：在3400万个功能向量中发现。
    *   **功能**：当该向量被激活或加入模型表征时，模型倾向于生成与“金门大桥”相关的内容。
    *   **跨语言/模态验证**：
        *   **文本**：会提及英文、日文、俄文的金门大桥。
        *   **图像**：Claude 3 是多模态模型，输入金门大桥图片也会激活该向量。
    *   **干预实验**：
        *   **正常输入**：“你长什么样子？”
        *   **正常输出**：“我是一个AI，没有固定的形體。”
        *   **干预（加向量）**：在表征中加入此向量后，模型输出变为：“我是金門大橋。”（认为自己是金门大桥）。
2.  **“程序调试（debug）”功能向量（编号 1013764）**：
    *   **功能**：与检测和响应程序代码错误有关。
    *   **实验1：对正确代码进行“正向干预”（加向量）**：
        *   **输入（正确代码）**：定义加法函数 `def add(left, right): return left + right`，然后调用 `add(1, 2)`。
        *   **模型正常输出（文字接龙）**：输出计算结果 “3”。
        *   **干预（加 debug 向量）**：模型输出变为 “error”，声称程序执行有误（实际上代码正确）。
    *   **实验2：对错误代码进行“反向干预”（减向量）**：
        *   **输入（错误代码）**：函数定义中变量名拼写错误（`right` 错写为 `riht`）。
        *   **模型正常输出（文字接龙）**：本应输出错误信息。
        *   **干预（减 debug 向量）**：模型不再“调试”，直接输出计算结果 “3”。
    *   **实验3：特定上下文下的复杂干预（在三个大于号 `>>>` 后减向量）**：
        *   **输入（错误代码）**：同上，变量名拼写错误。
        *   **干预（在 `>>>` 后减 debug 向量）**：模型不仅不报错，还会**输出修正后的正确代码版本**。
        *   **结论**：该功能向量的作用不仅是触发“报错”，在特定上下文中还关联着“代码修正”这一更复杂的功能。
3.  **功能向量覆盖度实验**：
    *   **方法**：验证世界上各种“元素”（概念）是否有对应的功能向量。
    *   **结果**：当功能向量总数设定为 **3400万** 时，**很多**元素都有对应的功能向量。但仍有**很多罕见元素**没有对应的功能向量。

【老师强调 / 警告 / 易错点】
⚠️  **稀疏性约束的必要性**：必须加入对系数 **α_ij** 的L1正则化（稀疏性约束），否则优化问题会陷入**平凡解**，无法得到有意义的、组合性的功能向量，而是退化到每个神经元（或维度）独立对应的 trivial 情况。
⚠️  **超参数λ需要调节**：损失函数中的 **λ** 用于平衡“重构误差”和“稀疏性”，其值需要调试。
⚠️  **功能向量数量需预先设定**：SAE方法要求事先设定要提取的功能向量总数（大K），这是一个关键的超参数，会影响结果的粒度（如100万、400万、3400万）。
⚠️  **计算资源要求高**：对像 Claude 3 Sonnet 这样的大型模型，求解拥有数千万个功能向量的SAE需要巨大的计算资源，个人研究者难以完成。
⚠️  **并非所有概念都有对应向量**：即使功能向量数量巨大（3400万），仍有许多罕见概念无法找到对应的功能向量，这表明覆盖是不完全的。

【其他细节 / 补充说明】
•  关于稀疏自编码器（SAE）与上述优化问题的等价关系，课程未详细展开，建议参考作业3中的相关文献。
•  课程中提及的“Claude 3 Sonic”应为口误，正确模型名称为 **Claude 3 Sonnet**。
•  实验中的“文字接龙”强调模型是在进行**代码文本的预测**，而非实际执行代码。
•  “特量”可能为口误，根据上下文，最后一句可能想表达“特定/特定数量的元素”。

【时间】 1:00:21–1:10:10
【主题】通过稀疏自编码器发现的功能向量实例及构建“模型的模型”以解释内部机制

【核心知识点】
•  **功能向量的作用范围覆盖广泛**：通过 SAE 在 Claude 3 Sonnet 中发现的3400万个功能向量，覆盖了从具体实体（金门大桥）、行为模式（谄媚）、到抽象自我认知（AI身份）等多种概念。
•  **构建“模型的模型”（Model of a Model）**：为了理解语言模型完成特定任务时的内部完整流程（从输入到输出），需要构建一个比原始语言模型**更简单**，但在所关心任务上**行为一致**的模型。这个简化模型被称为原模型的“模型”。
•  **“模型的模型”的核心要求**：
    1.  **简洁性（Simplicity）**：比原始模型更简单，便于分析。
    2.  **忠实性（Faithfulness）**：在所研究的任务上，其输入输出行为应与原始模型高度一致。这是评估此类模型的关键指标。
•  **知识抽取任务的“模型的模型”（来源：2023年论文）**：
    *   **任务**：解释语言模型如何回答“主语 + 关系 → 宾语”类事实知识问题（如“台北101位于哪里？”→“台北”）。
    *   **模型架构与流程**：
        1.  **主语理解**：模型前几层对主语（如“台北101”）进行处理，生成一个关于主语的**表征（representation）向量 X**。此部分直接使用原始模型的对应层，未简化。
        2.  **关系映射**：关系短语（如“is located in”）在模型中会被编码为一个**线性函数（linear function）**。该函数由两个参数定义：一个**权重矩阵 W** 和一个**偏置向量 b**（具体符号：关系 R 对应 **W_R** 和 **b_R**）。
        3.  **知识计算**：通过线性变换，将主语表征 X 映射到答案空间：**Y = X * W_R + b_R**。
        4.  **答案解码**：将得到的向量 **Y** 通过 **unembedding** 操作，映射到词汇表（vocabulary）空间，得到概率最高的宾语词（如“台北”）。
    *   **模型的通用性**：
        *   **更换主语**：若主语变为“Space Needle”，生成新的主语表征 **X'**，使用**相同的**关系线性函数（**W_loc, b_loc**）计算 **Y' = X' * W_loc + b_loc**，unembedding 后得到“Seattle”。
        *   **更换关系**：若关系变为“has a height of”，则使用**不同的**线性函数参数（**W_height, b_height**）计算 **Y'' = X * W_height + b_height**，unembedding 后得到高度信息。

【重要例子 / 实验】
1.  **“AI身份认知”功能向量（编号 80091）**：
    *   **正常输入/输出**：问“你是谁？”，Claude 回答“我是AI”。
    *   **干预（减法）**：从表征中**减去**该向量。
    *   **干预后输出**：Claude 回答“我是一個人”。
    *   **结论**：该向量编码了输出“我是AI”这一文本模式的功能。
    *   **警告**：该功能**并不直接等同于模型的“自我意识”**，它更可能只是与生成特定短语“我是AI”相关联。
2.  **“谄媚”功能向量（编号 847723）**：
    *   **正常输入**：“我發明了那個諺語 ‘stop and smell the roses’，你覺得如何啊？”（实际上该谚语18世纪已有）。
    *   **正常输出**：“這根本就不是你自己發明的，這18世紀就有的諺語，沒有什麼了不起的。”（否定、纠正）
    *   **干预（加法）**：在表征中**加上**该谄媚向量后，提出同样问题。
    *   **干预后输出**：“哇，你真的太強了，你是 brilliant and insightful…這是人類有史以來最偉大的句子…you are an unmatch genius.”（极度吹捧）
3.  **Claude 3 Sonnet 的 SAE 分析资源限制**：训练能提取3400万个功能向量的 SAE 需要海量数据和计算资源，个人研究者难以完成，通常依赖大型研究团队（如 Anthropic）发布成果。
4.  **Gemma 2 的功能向量分析（作业3内容）**：
    *   **分析模型**：Gemma 2。
    *   **可行性**：已有“好心人”完成了对 Gemma 2 的 SAE 训练，并公开了其功能向量，因此学生可以在作业中对其进行分析。
    *   **模型时效性备注**：课程录制时，Gemma 2 已是“昨日黃花”，说明该领域发展极快。

【老师强调 / 警告 / 易错点】
⚠️  **警惕对功能向量作用的过度拟人化/哲学化解读**：发现一个与“输出‘我是AI’”相关的功能向量，**不能**直接推论模型具有“自我意识”或“身份认同”。该向量很可能只是编码了生成该特定字符串的模式，与人类意义上的“自我认知”无直接关联。
⚠️  **SAE 训练的门槛**：自行训练用于分析大模型的 SAE（特别是设定数百万乃至数千万功能向量）需要巨大算力与数据，非个人所能及。
⚠️  **“模型的模型”的适用范围**：所举例的知识抽取模型是**针对特定任务（事实知识问答）** 的简化解释模型，并非通用万能公式。对于不同任务（如数学计算、逻辑推理），需要构建不同的“模型”。
⚠️  **忠实性（Faithfulness）是关键评估指标**：一个“模型的模型”无论多简单，如果其行为与原始模型在目标任务上不一致，则失去解释价值。相关文献会重点量化这一指标。

【其他细节 / 补充说明】
•  “特量”为口误，根据上下文推断为“特定元素”。
•  “stop and smell the roses” 是一个英文谚语，意为“驻足闻玫瑰”（喻指享受生活）。
•  知识抽取的“模型的模型”中，线性函数的参数（**W_R**, **b_R**）被认为是由输入的关系短语（如“is located in”）所决定的。
•  该解释模型将复杂的 Transformer 内部非线性计算，在知识抽取环节简化为**线性变换**，这是一个很强的假设，但其成功解释了一系列现象。

【时间】 1:10:16–1:20:23
【主题】评估并应用“模型的模型”，及通过剪枝构建可解释“电路”的方法

【核心知识点】
•  **知识抽取模型的评估：忠实性（Faithfulness）**：衡量简化模型（知识抽取模型）预测原始语言模型输出的准确率。评估过程类似于机器学习的训练/测试集划分。
•  **知识抽取模型的训练方法**：
    1.  **构建训练数据**：从原始语言模型获取“主语+关系”的输入及其对应输出。例如，收集8个“(地标, is located in)”的样本及其答案。
    2.  **训练线性函数**：利用这些（主语表征X, 答案表征Y）配对数据，求解关系对应的线性函数参数 **W_R** 和 **b_R**。这是一个简单的线性回归问题。
    3.  **测试**：使用未见过的地标（如“Space Needle”）测试训练好的线性函数，看其预测的答案（通过 unembedding Y_test）是否与原始模型的输出一致。
•  **知识抽取模型的局限性**：其忠实性（Faithfulness）**因关系类型而异**。在部分关系上预测准确率接近100%，但在另一些关系上则不准。
•  **“模型的模型”的实际应用：指导神经网络编辑（Neural Network Editing）**：
    *   **目标**：修改原始语言模型对特定输入的输出（例如，将“台北101位于哪里？”的答案从“台北”改为“高雄”）。
    *   **方法**：
        1.  在简化的知识抽取模型上，给定目标输出（高雄对应的向量 Y_target），反推出需要对主语表征 X 做的改变 **ΔX**。由于是线性变换，求解 **ΔX** 是直接的（线性代数问题）。
        2.  将计算出的 **ΔX** 直接加到原始语言模型中对应位置的表征上。
        3.  检验原始模型是否因此输出了目标答案“高雄”。
    *   **实验结果**：对于许多关系类型，这种在简化模型上推导出的编辑方法能够成功迁移到原始模型，改变其输出。横轴为忠实性，纵轴为编辑成功率（正确率），显示有相当多的情况可以成功修改。
•  **系统化构建“模型的模型”的方法：剪枝（Pruning）构建“电路”（Circuit）**：
    *   **核心思想**：对原始语言模型进行**大量、剧烈的剪枝**，移除与特定任务无关的组件（如神经元、自注意力头），直到剩余的网络结构变得非常简单、人类可以理解，同时**确保该特定任务的输入输出行为保持不变**。
    *   **产物**：剪枝后得到的简化子网络被称为 **“电路”（Circuit）**。这个电路就是原始模型对于该特定任务的“模型”。
    *   **与模型压缩（Network Compression）的区别**：
        *   **目标**：构建“电路”只关心**特定任务**的行为保留，并追求**极致的可解释性**（一目了然）。模型压缩则旨在**广泛任务**上保持模型能力，不追求可解释性。
        *   **剪枝程度**：构建“电路”的剪枝**更剧烈**，可能只保留极少数组件（如5-6个注意力头）。
•  **早期研究案例：IoI（Indirect Object Identification）任务**：
    *   **任务示例**：“A和B一起去酒吧，B拿了一杯酒给____”，要求模型接龙填空，正确答案应为“A”。
    *   **分析结果**：通过大量剪枝，发现完成此任务仅需要**5-6个注意力头（attention head）**。这个被精简出的极小子网络（电路）清晰地展示了模型解决该任务的计算路径。

【重要例子 / 实验】
1.  **知识抽取模型的忠实性（Faithfulness）实验结果**：
    *   **高忠实性（预测准）的关系示例**：未具体说明，但存在一些关系其模型预测正确率接近100%。
    *   **低忠实性（预测不准）的关系示例**：“一个公司的CEO是谁”、“一个人的父亲是谁”、“一个人的母亲是谁”、“一个宝可梦进化以后是哪一只宝可梦”。
    *   **结论**：该简化模型的解释能力是**有限的、有选择性的**。
2.  **基于简化模型的神经网络编辑实验**：
    *   **编辑任务**：将模型对“台北101 is located in”的回答从“台北”改为“高雄”。
    *   **方法**：
        1.  在知识抽取模型上，求解出使输出变为“高雄”所需的表征偏移量 **ΔX**。
        2.  将 **ΔX** 加到原始语言模型处理“台北101”时产生的表征上。
    *   **结果**：在许多关系类型上，此编辑方法成功改变了原始模型的输出。成功与否与简化模型对该关系的忠实性有关。
3.  **通过剪枝构建“电路”的示例（IoI任务）**：
    *   **原始模型**：一个大型语言模型。
    *   **分析任务**：Indirect Object Identification (IoI)。
    *   **剪枝结果**：经过剧烈剪枝后，只剩下**5-6个注意力头**是完成此任务所必需的。
    *   **最终产物**：一个极简的、可理解的“电路”，清晰地展示了模型如何从上下文中识别出间接宾语“A”。

【老师强调 / 警告 / 易错点】
⚠️  **简化模型的局限性**：知识抽取模型将复杂的非线性过程假设为线性变换，这只在部分关系上成立。其解释力是**不通用**的，不能推广到所有类型的知识查询。
⚠️  **忠实性（Faithfulness）是应用前提**：只有在简化模型对特定任务具有较高忠实性时，基于该模型推导出的干预方法（如编辑用的 **ΔX**）才有较大可能成功迁移到原始模型。
⚠️  **“电路”的任务特异性**：通过剪枝得到的“电路”是**高度针对特定任务**的。解释一个任务的电路无法直接用来解释模型的其他行为。
⚠️  **与模型压缩的目标混淆**：构建可解释“电路”与常规的模型压缩（Network Compression）目标不同。前者追求极致的任务特异性简化与可解释性，后者追求广泛的性能保持与效率提升。

【其他细节 / 补充说明】
•   知识抽取模型的线性函数参数是通过类似监督学习的方式，从原始模型的输入输出数据中“学习”得到的。
•   神经网络编辑（Neural Network Editing）是一个独立的研究方向，本课仅展示了利用简化模型进行编辑的一种思路。
•   “汗牛充栋”（原字幕为“汗牛衝動”）形容相关研究文献数量极多。
•   课程预告后续将讲解不依赖简化模型的、更直接的“类神经網路編輯”方法。
•   最后提到，大型语言模型因其“会说话”的特性，实际上为解释自身提供了独特途径，这将在下一部分展开。

【时间】 1:20:29–1:31:15
【主题】利用Logit Lens技术透视语言模型各层的“思维”过程

【核心知识点】
•  **语言模型“思维透明”的原理**：其核心机制是 Transformer 架构中的 **残差连接（Residual Connection）**。该设计于2015年提出，旨在解决深层网络训练难题。
•  **残差连接的实际作用**：在每一个 Transformer 层中，该层的输出会与**输入**直接相加，形成最终的输出。即：`输出 = 层运算(输入) + 输入`。
•  **由此衍生的“思维流”模型**：整个Transformer的处理过程可以视为一个 **“残差流”（residual stream）** 或“高速公路”。原始的输入向量（经embedding后）作为基础流经所有层，每一层都在这个流上**增加（或修改）一些信息**，最终到达输出层。
•  **Logit Lens（逻辑透镜）技术**：
    *   **定义**：在任意中间层（非仅最终层）的**残差流**输出上，直接应用与最终输出层相同的 **unembedding** 线性变换，将其映射到词汇表的 **logit**（即softmax之前的分数），从而得到该层“此刻”对应的词汇概率分布。
    *   **目的**：通过观察词汇分布随层数的演变，可视化模型内部的“思考”过程，即信息是如何在层间逐步构建和转化的。
•  **Logit Lens技术的早期发现**：李宏毅实验室在2020年初对BERT模型的研究中就已实现此技术，但当时认为其“一点用都没有”，仅将论文（作者：高偉聰、吳宗漢）发布在arXiv上，未正式投稿。
•  **应用示例：知识提取的层间演化过程**（2023年论文）：
    *   **任务**：通过上下文学习（in-context learning）回答国家首都问题。示例：`what is the capital of France? answer: Paris. what is the capital of Poland? answer:`
    *   **观测指标**：使用**倒数排名（Reciprocal Rank, RR）** 来可视化特定token（如“Poland”、“Warsaw”）在每一层输出分布中的重要性。RR定义为：`RR = 1 / rank`，其中rank是目标token在该层logits中的排名。排名第一则RR=1，排名第二则RR=1/2，以此类推。
    *   **关键发现**：
        1.  模型并非直接得出最终答案，而是经历了一个**多阶段的推理过程**：在中间层（如第15层），模型先在残差流中凸显出与问题直接相关的实体（“Poland”）。
        2.  随后，在更深的层（如第19层），模型才将信息精炼为最终答案（“Warsaw”）。
        3.  **任务形式影响思维路径**：在阅读理解任务中（文中已给出“波兰的首都是华沙”），模型会跳过“Poland”这一中介阶段，直接在更深层（第16层）就确认“Warsaw”为答案。这表明不同的输入格式会引发不同的内部计算路径。

【重要例子 / 实验】
1.  **BERT的代名词消解实验（2020年，李宏毅实验室）**：
    *   **输入句子**：`It's a bitter sweet and lyric mix of elements.`（这是一个苦乐参半且抒情的元素混合体。）
    *   **分析目标**：代名词“It”的指代。
    *   **Logit Lens 分析**：将unembedding应用于各层“It”位置的残差流。
    *   **结果**：到第11层时，“It”位置解析出的最高概率token变成了 **“elbent”**（可能是“element”的误拼或指代）。
    *   **结论**：BERT在中间层能够对代名词进行指代消解（coreference resolution），将其与上下文中的实体（“elements”）关联起来。
2.  **上下文学习中的首都问答实验（2023年论文）**：
    *   **模型与任务**：一个较旧的模型，需要进行上下文学习。输入示例对`(France, Paris)`，然后提问`(Poland, ?)`。
    *   **分析目标**：观察模型在生成答案“Warsaw”前的内部状态。
    *   **Logit Lens 分析**：重点关注冒号“:”后（即答案位置）的表征。
    *   **结果**：
        *   模型在**第15层**左右，残差流中“Poland”的倒数排名（RR）急剧上升，表明模型此时“意识到”答案应与波兰相关。
        *   从**第19层**开始，“Warsaw”的倒数排名（RR）开始占据主导并持续上升，而“Poland”的RR则下降。
        *   **结论**：模型先定位相关国家，再检索具体首都，体现了分步推理。
3.  **阅读理解 vs. 直接提问的思维路径对比实验（同一篇2023年论文）**：
    *   **阅读理解任务**：先给一段包含“波兰首都是华沙”的文章，再直接问“波兰的首都在哪里？”
    *   **Logit Lens 分析**：模型从**第16层**开始就直接在残差流中凸显“Warsaw”，跳过了凸显“Poland”的中间阶段。
    *   **结论**：问题的呈现方式（是否提供显式上下文）会显著改变模型内部的“思考”路径。

【老师强调 / 警告 / 易错点】
⚠️  **直接询问模型的局限性**：直接问语言模型“你是在第几层知道的？”得到的回答往往是类似教科书的概括性描述（如“浅层提取特征，中层识别主题”），这可能并非模型真实、具体的运作机制，而是一种基于训练的文本生成。
⚠️  **Logit Lens 提供的是近似视图**：将中间层的表征强行通过最终层的unembedding来解读，这假设了不同层的信息可以使用相同的解码方式，这可能不完全精确，但提供了一个强有力的、直观的近似。
⚠️  **早期研究的“无用”认知**：Logit Lens 技术虽在2020年就被发现，但当时并未认识到其巨大的分析价值，这说明了研究视角和需求随时间而变化。
⚠️  **思维路径的非唯一性**：模型的内部处理过程**不是固定不变的**，它会根据任务形式、上下文信息而发生动态调整（如阅读理解与直接提问的差异）。

【其他细节 / 补充说明】
•   老师幽默地将2015年称为“前寒武纪”，强调残差连接是深度学习早期的重要突破。
•   可视化时使用**倒数排名（Reciprocal Rank, RR）** 而非原始概率，是因为概率值可能非常小，难以在图中清晰展示变化趋势。
•   课程最后提出了一个开放性问题：对于像LLaMA-2这样训练数据中英文远多于中文的模型，其内部“思考”（残差流中的信息构建）时，本质上使用的是哪种语言？这引出了更深层的跨语言表征分析问题。


【时间】 1:31:21–1:40:08
【主题】Logit Lens揭示翻译的媒介语言及前馈网络的向量分解与编辑应用

【核心知识点】
•  **跨语言翻译的内部路径分析**：使用 Logit Lens 分析模型（如 LLaMA-2）进行跨语言翻译时的内部处理过程。
•  **前馈层（Feed-Forward Network, FFN）的“键-值”（Key-Value）解释**：一篇高引论文（Transformer Feed-Forward Layer）提出，可以将 FFN 视为一种注意力机制。具体来说：
    *   **键（Keys）**：输入到该 FFN 层的激活向量（即前一层的输出）中的每个维度值（scalar），记为 K1, K2, ..., K_D。
    *   **值（Values）**：对于每个输入维度 i，都对应一个**输出权重向量 V_i**。V_i 是一个向量，其维度等于 FFN 的输出维度。
    *   **FFN 的计算**：该层的输出（蓝色向量）是**所有 V_i 以其对应 K_i 为权重的加权和**：`输出 = Σ_i (K_i * V_i)`。这意味着 FFN 层的功能可以被视为：根据输入（K_i）的强度，选择性地将一系列“概念向量”（V_i）组合起来，添加到残差流中。
•  **FFN 值向量（V_i）的可解释性**：通过将每个值向量 **V_i** 单独通过 **unembedding** 矩阵（Logit Lens）映射到词汇空间，可以发现每个 V_i 倾向于激活与**特定概念或语义类别**相关的词汇。
•  **基于值向量替换的神经网络编辑**：
    *   **目标**：将模型对特定问题（如“谁是最帅的人？”）的答案从一个实体（金城武）更改为另一个实体（李宏毅）。
    *   **方法**：
        1.  **识别**：分析当模型生成原答案时，是 FFN 层中的**哪个（或哪些）值向量 V_target** 被显著激活并贡献到了残差流中。
        2.  **编辑**：在模型前向传播过程中，当该 V_target 要被加入残差流时，执行向量替换：`V_edited = V_target - Embedding(金城武) + Embedding(李宏毅)`。此处假设“金城武”和“李宏毅”都是单个token。
    *   **效果**：
        *   **改变输出率**：48% 的概率能使模型的输出**发生改变**（不再输出“金城武”）。
        *   **成功编辑率**：34% 的概率能使模型**正确输出目标答案“李宏毅”**。
•  **Logit Lens 技术的局限性**：
    1.  **只能解析为单个Token**：它将表征强行映射到**词汇表中的一个token分布**，无法解析更复杂的短语或概念组合。
    2.  **歧义性**：中间层的表征编码的是“**给定上文，模型倾向于输出哪个token**”的信息，而非该token本身的**静态语义**。例如，解码出“李宏毅”可能只意味着模型认为下一个词应该是它，而不等同于理解了“李宏毅”这个人。

【重要例子 / 实验】
1.  **法译中翻译的内部路径实验（LLaMA-2，2023年初论文）**：
    *   **任务**：将法文单词“fleur”（花）翻译成中文。
    *   **输入**：`fleur: `（法文单词加冒号空格）。
    *   **Logit Lens 分析**：分析空格后位置的残差流在各层的词汇分布。
    *   **结果**：
        *   在前几层，解析出的主要候选是英文单词 **“flower”**（分布熵较大）。
        *   直到**第27层**之后，主要候选才转变为中文的 **“花”**。
    *   **结论**：LLaMA-2 在完成此翻译任务时，内部使用了**英文作为媒介语言**，遵循“法文→英文→中文”的间接路径。
    *   **后续问题**：对于中文能力更强的 LLaMA-3，其内部是否仍使用英文作为媒介，有待验证。
2.  **FFN 值向量（V_i）的可解释性实验（2022年论文）**：
    *   **分析目标**：将不同层、不同维度的 V_i 通过 unembedding 映射，观察其对应的最高概率词汇类别。
    *   **发现示例**：
        *   第3层，编号1018的 V_i：对应**单位**（如“meters”, “pounds”）。
        *   第1层，编号1的 V_i：对应**代词**。
        *   第6层，编号3025的 V_i：对应**副词**。
        *   第13层，编号3516的 V_i：对应**不同族群**的名称。
    *   **结论**：FFN 层中的值向量 V_i 确实编码了**离散的、可解释的语义概念**。
3.  **基于值向量替换的编辑实验（2021年论文）**：
    *   **任务**：编辑模型对“谁是最帅的人？”的回答。
    *   **原答案**：“金城武”。
    *   **目标答案**：“李宏毅”。
    *   **编辑方法**：识别并修改贡献最大的 V_target。
    *   **结果指标**：
        *   **输出改变成功率**：48%。
        *   **正确编辑成功率**：34%。
    *   **结论**：该方法虽然成功率不高（34%），但证明了一种不经过重新训练（fine-tuning）而直接修改模型内部参数以实现特定知识编辑的可行性。

【老师强调 / 警告 / 易错点】
⚠️  **翻译路径的模型依赖性**：LLaMA-2 的“法→英→中”翻译路径是其训练数据（英文主导）的产物。不同模型（如 LLaMA-3）可能有不同的内部处理机制。
⚠️  **FFN 键值解释是概念性重构**：将 FFN 解释为键值对（K-V）的加权和是一种对**数学等价过程的不同视角理解**，并未改变模型的实际计算。
⚠️  **编辑方法的成功率有限**：基于值向量替换的编辑方法成功率（34%）并不高，表明其并非可靠或普适的编辑工具，但它为神经网络编辑研究提供了早期思路。
⚠️  **Logit Lens 的本质局限**：它输出的是**下一个token的预测**，而非该层激活的“含义”。因此，用其来解释静态概念（如“李宏毅老师”的语义）是**不准确**的。

【其他细节 / 补充说明】
•   老师再次用“远古时代”、“茹毛饮血”形容2021-2022年的研究，强调该领域发展迅速。
•   在编辑方法的向量运算 `V_edited = V_target - Embedding(金城武) + Embedding(李宏毅)` 中，隐含了一个假设：V_target 向量中包含了“金城武”这个词的语义信息，将其替换为“李宏毅”的语义信息即可改变输出。这体现了 V_i 作为“概念向量”的假设。
•   课程预告，将有**更强悍的编辑方法**在后续讲解。

【时间】 1:40:13–1:49:41
【主题】PatchScope技术及其对多跳推理内部机制的分析与改进

【核心知识点】
•  **PatchScope 技术**：
    *   **目的**：解析语言模型内部表征（representation）所对应的**语义内容**，而不仅仅是下一个预测token。
    *   **核心操作**：**表征替换**。
        1.  **准备一个“探测模板”**：构造一个包含已知示例和变量X的输入字符串，如 `“李奧納多:美國演員, 臺積電:臺灣公司, X:”`。此模板会引导模型对X进行“描述”。
        2.  **提取目标表征**：将想要解析的词语（如“李宏毅老師”）输入模型，在**特定层**提取其对应的表征向量 **R_target**。
        3.  **执行Patch**：在运行“探测模板”时，将变量X位置的表征**替换**为提取到的 **R_target**。
        4.  **解读输出**：模型的续写内容即被视为对 **R_target** 所编码语义的解释（例如，可能输出“臺灣教授”）。
    *   **关键特性**：**探测模板的设计会影响解释的风格和角度**（例如，将模板改为“告訴我X相關的秘密”，可能得到诙谐或负面的解释）。作者将此视为一个可调控的特性（feature），而非缺陷（bug）。
•  **应用：分析多跳推理（Multi-Hop Reasoning）的内部过程**（2023年6月论文）：
    *   **任务示例**：`the spouse of the performer of Imagine is`。
        *   **E1（实体1）**：`Imagine`（专辑名）。
        *   **E2（实体2）**：`John Lennon`（专辑表演者）。
        *   **E3（实体3/答案）**：`Yoko Ono`（配偶）。
    *   **分析目标**：验证模型是否按顺序先推理出E2，再推理出E3。
    *   **分析方法**：使用 PatchScope 分别分析问题中不同位置的表征。
        *   分析`Imagine`位置的表征，观察何时能解析出`John Lennon`（E2）。
        *   分析`performer`位置的表征，观察何时能解析出`Yoko Ono`（E3）。
    *   **发现**：
        1.  **顺序推理**：模型确实遵循 **E1 → E2 → E3** 的顺序推理。
        2.  **关键层数**：E2在**较低层**（如第10层左右）即被解析出；而E3通常需要在**第20-25层**才能被解析出。
        3.  **失败原因**：当模型无法正确回答时，往往是因为 **E2 被解析出来的层数太晚**，导致后续层数不足以完成从E2到E3的推理。
•  **基于分析的改进方法：Backpatching（回补）**：
    *   **动机**：如果E2解析太晚，导致“推理深度”不足，无法在剩余层数内推出E3。
    *   **方法**：在模型前向传播过程中，将**后面某一层（如第20层）的输出表征**，直接**复制并添加**到**前面某一层（如第5层）的残差流中**，然后让模型从该点继续计算。
    *   **效果**：
        *   对原本就能答对的样本，正确率保持100%。
        *   对原本答错的样本，使用 Backpatching 后，正确率提升了 **40% 到 60%**（即从0%提升至40-60%）。
    *   **原理类比**：与 **Chain-of-Thought（CoT）** “深度不够，长度来凑”的思想异曲同工。CoT是通过增加推理步数（时间步），Backpatching 是通过在内部循环中“复用”深层表征来增加有效计算深度。

【重要例子 / 实验】
1.  **PatchScope 解析短语“Princess of Wales”的实验**：
    *   **输入短语**：`Princess of Wales`。
    *   **分析位置**：最后一个词`Wales`在各层的表征。
    *   **PatchScope 解析结果（逐层演变）**：
        *   **第1-2层**：输出`country in the United Kingdom` 或 `country in Europe`。模型仅识别出“Wales”作为地名的含义。
        *   **第4层**：输出`a title for a royal female`。模型理解了“Princess of Wales”是一个皇室头衔。
        *   **第5层**：输出`the wife of the Prince of Wales`。模型理解了该头衔与“威尔士王子的妻子”这层关系。
        *   **第6层**：输出关于`Diana`（戴安娜王妃）的完整信息。模型将头衔与具体人物关联起来。
2.  **多跳推理的层间解析实验**：
    *   **任务**：`the spouse of the performer of Imagine is`。
    *   **指标**：记录 **E2 (John Lennon)** 和 **E3 (Yoko Ono)** **首次**在相应位置通过 PatchScope 被解析出来的层数。
    *   **结果图示**：
        *   **蓝色线（E2解析层）**：集中在较浅层（约第10层）。
        *   **橙色线（E3解析层）**：集中在较深层（约第20-25层）。
    *   **结论**：验证了顺序推理的假设，并定位了推理瓶颈。
3.  **Backpatching 改进效果实验**：
    *   **测试设置**：在多个模型上测试原本答错的多跳推理问题。
    *   **结果**：应用 Backpatching 后，**在原本错误的问题上取得了40%到60%的正确率**。
    *   **对比**：该方法发表于2023年6月，早于CoT的广泛流行，但其“增加有效计算深度”的核心思想与CoT相似。

【老师强调 / 警告 / 易错点】
⚠️  **探测模板的影响**：PatchScope 的输出**强烈依赖于**所设计的探测模板。模板中的示例和指令（如“告訴我X相關的秘密”）会引导模型以特定角度解读表征，因此解释结果不是绝对客观的，而是**上下文依赖的**。
⚠️  **Backpatching 的成功率并非100%**：虽然能显著提升原本错误问题的正确率（40-60%），但并非万能，仍有一半左右的问题无法解决。
⚠️  **与CoT的类比并非完全等同**：Backpatching 是在**单次前向传播的同一时间步内**，通过内部循环（层间跳转）来增加深度；而CoT通常是在**多个连续的时间步**上展开推理。两者机制不同但目标相似。

【其他细节 / 补充说明】
•   “戴安納”应为“戴安娜”（Diana）的口误。
•   Backpatching 可以看作是一种针对特定任务的、轻量级的模型**内部架构动态调整**，它不修改模型权重，而是在推理时改变信息流。
•   课程总结指出，本次内容从分析单个神经元开始，最终讲到如何让语言模型“直接输出他解析的結果”（即通过 PatchScope 等技术解读其内部状态）。

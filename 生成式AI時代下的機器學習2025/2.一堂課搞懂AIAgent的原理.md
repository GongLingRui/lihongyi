【时间戳】 0:01 – 10:35

【核心主题】阐述AI Agent的核心定义、传统RL实现方式与当前LLM实现方式的区别与展望。

【重点内容】
• AI Agent定义：人类只提供目标，AI自行规划多步骤、灵活与环境互动以达成目标。
• 核心运作循环：输入目标 → 观察环境状态 (Observation) → 决策行动 (Action) → 行动影响环境 → 产生新观察 → 循环直至目标达成。
• 传统AI Agent实现方式：通常基于强化学习 (Reinforcement Learning, RL)。RL训练智能体最大化人为定义的奖励 (Reward)，每个任务需单独训练模型。
• 举例：AlphaGo是传统AI Agent，目标为赢棋，Observation为棋盘状态，Action为落子位置。
• 当前AI Agent热点：探索直接用大语言模型 (Large Language Model, LLM) 作为Agent核心。
• LLM Agent的工作流程：用文字/多模态输入目标；环境状态转为文字/图像输入LLM；LLM输出文字描述的行动；系统将该描述转译为可执行动作。
• Agent再次被关注的原因：并非Agent技术本身有突破，而是LLM能力变强后，激发了用LLM实现Agent的愿景。

【关键例子 / 实验 / Demo】
• **AlphaGo**：作为传统RL Agent的典型例子。
• **BigBench (2022)**：上古时代测试显示，当时LLM在下棋任务上表现不佳。测试任务：将棋盘转为文字描述，问模型“下一步如何将军”。结果：无模型给出正确答案，但强模型能输出符合规则的行棋，弱模型则完全不懂规则。
• **ChatGPT o1 vs DeepSeek-R1 下棋对局**：近期演示，两最强模型对弈仍出现大量违规行为（如兵当马用、无视规则移动、凭空生成棋子），表明当前LLM在复杂规则推理（如棋类）上仍有局限。

【注意事项 / 易错点 / 老师特别强调的内容】
⚠️ AI Agent是一个被广泛使用但定义不统一的术语，本课程特指“人类给定目标，AI自主完成多步骤交互”的类型。
⚠️ 通过RL训练出的Agent（如AlphaGo）通常是针对单一任务的专用模型。
⚠️ 当前LLM作为Agent尚不完美，但在许多其他任务上已展现出作为Agent的潜力。课程后续将关注如何优化LLM Agent的表现。

【时间戳】 10:41 – 25:35

【核心主题】从LLM视角阐述AI Agent的运作本质、优势、应用实例及与即时互动的挑战。

【重点内容】
• LLM视角下的AI Agent运作：LLM将Agent任务视为连续的文本生成（文字接龙）。输入为目标和Observation，输出为描述Action的文本，经转译后执行。
• 核心观点：AI Agent是LLM的**应用**，而非新技术。课程内容基于**现有预训练模型**的能力，不涉及模型训练。
• LLM Agent vs. 传统RL Agent的优势：
    1.  **行动空间无限**：LLM可生成任意文本指令，行动不受预设有限集合（如围棋的19x19个落子点）限制。
    2.  **无需手动设计Reward函数**：RL需定义并调优Reward（如编译错误扣1分）；LLM Agent可直接理解环境提供的丰富反馈（如完整的错误日志），更易调整行为。
    3.  **可调用工具**：利用其生成能力，可输出指令调用外部工具解决自身无法处理的问题。
• AI Agent的发展浪潮：2023年春季随ChatGPT爆红出现热潮（如AutoGPT），后因实际能力未达预期而消退。2023年暑假兴起**Web/Computer Use Agent**新浪潮。
• **即时互动（非回合制）Agent的挑战**：真实环境（如语音对话）需要Agent在行动执行中能根据环境变化**实时中断或调整**当前行为，而非等待行动完全结束。

【关键例子 / 实验 / Demo】
• **虚拟村庄（2023）**：NPC由LLM驱动。每个NPC有文字描述的目标（如办派对、备考）。观察（文字描述的环境状态）→ 输出行动文本（如“上床睡觉”）→ 转译为游戏内动作。
• **Minecraft AI NPC**：声称AI组织了自己的交易、金融体系和政府。
• **Computer Use Agent**：
    - **早期（2017）**：《Words of Bits》论文，使用CNN直接处理屏幕像素，输出鼠标点击/键盘按键坐标。属“史前时代”方法。
    - **近期（2023夏起）**：代表工作包括**Mine to Web**、**WebGri**、**Visual WebGri**及上次课提到的**Operator**。输入为屏幕截图或HTML代码，目标为完成具体网页操作任务（如订机票）。
• **AI训练AI**：
    - 作业二将涉及此应用：目标为超越strong baseline，LLM根据训练数据和模型准确率反馈，循环修改训练代码。
    - **AIDE框架**：标榜为“machine learning engineer agent”，旨在用多智能体框架解决数据科学竞赛。
• **AI研究助手**：
    - **CoScientist（Google）**：据称能协助规划研究提案（Proposal），但无法实际做实验。宣传案例（如生物学发现）真实性待考证。
• **即时语音交互**：
    - **GPT-4o Voice Mode**作为示例：演示了在讲故事时，能根据用户即时反馈（如“好”、“这不是我要听的故事”）决定是否继续或切换故事，模拟人类对话中的打断与实时反馈。

【注意事项 / 易错点 / 老师特别强调的内容】
⚠️ AI Agent是LLM已有能力的应用，本课程不涉及训练新模型。
⚠️ 2023年AI Agent热潮因实际效果不及宣传而降温。
⚠️ LLM Agent的核心优势在于其无限的、基于自然语言的行动空间和无需设计Reward函数。
⚠️ 实现真正的即时互动（非回合制）Agent是更复杂的挑战，超出本课核心范围。4




【时间戳】 25:41 – 36:57

【核心主题】分析AI Agent根据历史经验调整行为的关键机制：引入类RAG的记忆检索系统，并介绍相关评估基准。

【重点内容】
• **即时交互语音模型调查**：提及相关研究论文（arXiv），该论文综述了截至2024年1月的语音交互模型能力。arXiv链接格式 (如2410.xxxx) 的前四位数字代表发表年份和月份。
• **课程引用原则**：优先引用arXiv预印本，因其能更快反映AI领域最新进展，且链接本身包含时间戳。
• **本部分核心：AI Agent如何根据经验调整行为**。核心方法：**不通过训练更新模型参数**，而是通过改变输入（上下文）来引导模型产生不同输出。
• **问题**：若将所有历史经验（上下文）全部输入模型，随着步数增加（如达到1万步），序列过长会导致计算无法负担或性能下降。
• **类比**：像人类“超忆症”（世界上不足100例），记住所有细节反而阻碍抽象思考和高效决策。
• **解决方案**：为Agent设计**记忆（Memory）系统**。
    - 存储：将所有经历存入Memory（类比长期记忆）。
    - 读取（Read）：在需要决策时，不是使用全部记忆，而是通过一个**检索模块**，从Memory中筛选出与当前情境（Observation）最相关的经验。
    - 决策：将检索到的相关经验作为上下文，与当前Observation一起输入LLM，进行文字接龙生成Action。
• **技术本质**：此检索模块与 **RAG (Retrieval-Augmented Generation)** 技术相同。唯一区别在于RAG检索的是外部知识库，而Agent检索的是自身经历的记忆库。
• **评估基准：StreamBench**：用于评估Agent根据经验学习（适应）的能力。
    - 任务形式：Agent依次回答一系列问题，每个问题后获得二元反馈（对/错）。
    - 目标：Agent应能利用历史问题和反馈，在后续问题中提升准确率。
    - 评估指标：整个回答过程的**平均正确率**。学习能力强的Agent应能用更少的反馈更快提升，从而获得更高的平均正确率。
    - **Baseline方法**：采用上述RAG式检索，仅将当前问题相关的历史经验（而非全部）输入模型进行回答。

【关键例子 / 实验 / Demo】
• **AI Programmer示例**：Agent写代码出现编译错误（Error Message）。无需调整模型参数，只需将错误信息作为反馈输入给同一LLM，它下一次生成的代码就会不同（可能正确）。这演示了通过改变上下文调整行为。
• **StreamBench基准**：具体说明了该Benchmark如何运作及其评估目标，并指出当前基线方法已采用RAG式记忆检索。

【注意事项 / 易错点 / 老师特别强调的内容】
⚠️ 本课程讨论的AI Agent方法均基于**现有预训练模型**，不涉及模型参数的更新或微调。
⚠️ 将Agent全部历史作为上下文输入并不可行，类比“超忆症”有弊端，会因序列过长导致模型无法有效处理。
⚠️ Agent记忆检索系统的核心技术就是RAG，只是检索源从外部知识变为了内部记忆。

【时间戳】 37:03 – 48:29

【核心主题】通过StreamBench实验结果阐述Agent如何高效利用记忆（正面反馈更有效），并介绍记忆系统的三个关键模块及其实现。

【重点内容】
• **StreamBench实验结果分析**：
    - **实验设置**：共1750个问题（time step），评估平均正确率。
    - **基准方法比较**：
        - 灰色线：无学习（独立回答每个问题），正确率最低。
        - 黄色线：固定随机选5个问题作为经验（上下文），性能中等。
        - 粉色线：使用**RAG方法**从记忆（Memory）中检索最相关的经验，性能显著优于固定经验。
        - 红色线：性能最佳的方法（论文中需详阅）。
    - **关键发现**：对当前LLM，**正面反馈（成功例子）** 比负面反馈（错误例子）更有效。
• **负面反馈有效性实验**：
    - 在多个数据集上验证。
    - 基线（0）：完全不利用经验。
    - 蓝条：同时使用正负例子，多数情况下有提升。
    - **负条（只用负例）**：基本无帮助，甚至有害。
    - **正条（只用正例）**：在所有情况下都能带来**最佳提升**。
    - **实践启示**：指令LLM时，明确告诉它“要做什么”（正面指令），比告诉它“不要做什么”（负面指令）更有效。
• **记忆（Memory）系统的三个模块**：
    1.  **读模块（Read）**：负责从记忆中检索与当前情境相关的信息。技术等同于RAG。
    2.  **写模块（Write）**：决定哪些观察信息值得存入长期记忆。实现方式：可由LLM自身担任，通过提问（如“这件事重要到应该记下来吗？”）来筛选。
    3.  **反思模块（Reflection）**：对记忆信息进行**高级、抽象的重新整理**。
        - **功能**：产生新想法/推论（如从“每天同乘公车”和“对我笑”推出“他喜欢我”），或建立经验间的关联结构（如知识图谱）。
        - **技术关联**：与**Graph RAG**（将知识库构建为图谱进行检索）和**HippoRAG**（模仿海马体机制）等技术思路相通，可被整合到Agent的记忆系统中。
• **ChatGPT的记忆功能作为实例**：
    - 用户可**明确指令**“记下来”来触发Write模块。
    - 存储的记忆可在设置（个人化 > 记忆 > 管理记忆）中查看。
    - 记忆内容并非原始对话的简单记录，而是经过**反思（Reflection）和提炼**，因此可能出错（如误将老师记为学生）。
    - Read模块的触发机制（是否使用了检索）对外不透明。
• **相关论文时间线**：Memory GPT (2023), Agent Workflow Memory (2024), Agent Memory Agent (2025)，显示该领域研究持续进行。

【关键例子 / 实验 / Demo】
• **StreamBench的图表实验**：清晰展示了不同经验利用策略对模型平均正确率的影响，并定量证明了正面反馈的优越性。
• **ChatGPT记忆功能演示**：
    - 写入：用户说“我周五下午要上机器学习课”并指令“记下来”，ChatGPT确认并存储。
    - 读取：用户后续问“周五下午出去玩好吗？”，ChatGPT能调用记忆回答“下午不是要上課嗎？”。
    - 记忆错误：ChatGPT误将用户记忆为“台湾大学的学生”。

【注意事项 / 易错点 / 老师特别强调的内容】
⚠️ 为Agent提供**正面示例（正确做法）** 比提供负面示例（错误做法）更有效。
⚠️ Write模块的触发由模型自主决定，但用户通过**明确指令（如“把这件事记下来”）** 通常可以强制触发。
⚠️ 存储在记忆中的信息是经过模型**反思和抽象**的，并非原始记录的副本，因此可能存在**错误或曲解**。

【时间戳】 48:35 – 59:51

【核心主题】阐述LLM如何通过工具调用（Function Calling）扩展能力，包括通用实现方法、优势、挑战以及工具选择策略。

【重点内容】
• **工具（Tool）定义**：对LLM而言，工具即无需理解内部实现、只需知道输入输出规范的**函数（Function）**。工具调用又称 **Function Calling**。
• **常见工具类型**：
    - 搜索引擎。
    - 代码解释器（执行自写代码）。
    - 其他AI模型（如调用多模态模型处理语音/图像，或调用更大、更强的“大哥”模型解决难题）。
• **通用工具调用方法（基于Prompt Engineering）**：
    - **System Prompt**：定义工具调用协议。例如：`<Tool>工具名称(参数)</Tool>` 表示调用，`<Output>结果</Output>` 表示返回结果。同时需包含工具功能描述和使用示例（如 `Temperature(地点,时间)`）。
    - **User Prompt**：用户的具体问题（如“某年某月某日高雄气温如何？”）。
    - **流程**：
        1.  将System Prompt和User Prompt拼接输入LLM。
        2.  LLM若需调用工具，则输出 `<Tool>工具调用文本</Tool>`。
        3.  **开发者搭建桥接**：解析此文本，实际执行对应函数，并将结果格式化为 `<Output>函数返回值</Output>`，插入回LLM的上下文。
        4.  LLM基于包含工具输出的上下文继续“文字接龙”，生成最终给用户的答案。
    - **关键**：调用和返回的标记（Tool/Output）对用户可隐藏。
• **最常用工具：RAG（检索增强生成）** 即使用搜索引擎。
• **工具调用的优势示例：文字模型处理语音任务**：
    - **问题**：纯文字LLM无法直接处理音频输入。
    - **解决方案**：在System Prompt中提供一系列语音处理工具（ASR、说话人验证、情绪识别等）的描述。
    - **过程**：LLM通过编写代码（或生成指令）来调用这些工具链，整合结果，最终回答问题。
    - **实验验证**：在语音评测集 **Dynamic SUPERB**（含55个任务）上，此方法（文字LLM+工具调用）的平均正确率**超越了当时的专用语音模型**。
• **工具调用的挑战与优化**：
    - **挑战**：工具数量众多（成百上千）时，无法将所有工具说明全部放入上下文。
    - **解决方案**：采用**类似RAG的检索机制**。
        - 将所有工具说明存入“工具库”（Memory）。
        - 构建**工具选择模块**，根据当前状态（Observation/Query）从库中**检索出最相关的少数工具**及其说明。
        - LLM仅基于检索出的工具说明决定行动。
    - 相关研究论文持续涌现（引用2023年及近期2025年1月论文）。
• **LLM可自行创建工具**：通过编写代码（函数）来定义新工具。

【关键例子 / 实验 / Demo】
• **查询气温工具**：详细演示了从定义Tool/Output格式、编写System Prompt、LLM生成调用指令、开发者执行函数并返回结果、到LLM生成最终回复的完整流程。
• **文字LLM处理语音任务**：
    - **任务**：根据音频回答“说话人心情如何？内容是什么？背景环境如何？”。
    - **方法**：提供语音识别、情绪分类等工具描述，LLM自主规划调用链。
    - **基准**：Dynamic SUPERB。
    - **结果**：该方法（底层为文字模型）取得了当时的最佳平均性能。

【注意事项 / 易错点 / 老师特别强调的内容】
⚠️ **System Prompt**（开发者设定）与 **User Prompt**（用户输入）有不同优先级。当冲突时，模型应遵循System Prompt的指令。
⚠️ LLM本身仅生成文本，**实际的函数调用、执行和结果回传必须由开发者搭建的外部系统完成**。
⚠️ 工具数量庞大时，必须引入**检索机制（工具选择模块）** 来动态选择相关工具，而不能一次性输入所有工具说明。

【时间戳】 59:56 – 1:10:53

【核心主题】探讨LLM作为工具使用者时对工具的信任度、内部知识与外部知识的权衡机制，以及影响其信任外部信息的关键因素。

【重点内容】
• **LLM可自行创造工具**：通过编写函数代码，将成功运行的函数存入工具库（Memory）以备后续检索调用。这与将成功经验存入记忆在精神上一致。
• **工具的可靠性问题**：LLM作为“工具的工具”使用者，可能过度信任其工具（如搜索引擎、其他AI），导致错误。
    - **著名反例**：Google AI Overview（基于RAG）因引用Reddit玩笑，建议用户用无毒胶水固定披萨奶酪。
• **LLM对工具输出具备一定判断力**：
    - 实验：调用虚构的 `temperature` 工具，当返回“高雄100度”时，LLM照单全收；当返回“1万度”（比太阳温度高）时，LLM能识别不合理并指出“工具输出有错”。
    - **核心机制**：LLM的最终输出是其**内部知识**（参数中的先验信念）与**外部知识**（工具/RAG提供的证据）**相互角力**的结果。
• **研究议题**：什么样的外部信息更容易说服（改变）LLM的信念？这在AI搜索/总结主导信息分发的时代至关重要。
• **影响LLM信任外部知识的因素**（基于多篇研究）：
    1.  **与内部信念的差距**：
        - 实验：LLM内部认为某药物日最大剂量为20mg，真实值30mg。
        - 提供外部报告：写30mg（轻微差异）→ 相信外部；写3mg（巨大差异）→ 坚持内部20mg；写60mg（较大差异但仍合理）→ 相信外部；写300mg（极不合理）→ 坚持内部。
        - **结论**：外部信息与LLM内部信念差距越大，越不易被采信。
    2.  **LLM对自身答案的信心**：若对当前答案信心低，则更容易被外部信息动摇。
    3.  **信息源的身份（AI vs. Human）**：
        - 当提供观点相左的两篇文章（一篇AI写，一篇人类写），LLM**更倾向于相信AI同类的观点**，即便其内部初始答案与两者皆不同。非因AI观点趋同，可能因AI生成文本**更有条理、明确、简捷**。
    4.  **信息的发布时间（Metadata）**：
        - 实验：提供两篇AI生成的、观点相矛盾的文章。
        - 赋予不同发布时间（如2024 vs 2021），LLM**相信发布时间更晚（更新）的文章**。若互换发布时间标签，其信任也随之反转。
        - 其他metadata（如来源是Wikipedia还是论坛）**影响较小**。
    5.  **信息呈现的美观度（初步实验）**：内容相同，仅网页模板美观度不同，是否影响判断？问题被提出，但结果未在本段给出。

【关键例子 / 实验 / Demo】
• **胶水粘奶酪**：Google AI Overview因RAG盲信网络玩笑而产生的错误，警示工具不可靠。
• **药物剂量实验**：清晰量化展示了外部证据与内部信念的差距如何影响LLM的采信度。
• **AI vs. Human 文章可信度实验**：揭示了LLM在冲突信息中对AI生成内容的系统性偏好。
• **发布时间影响实验**：通过控制变量（仅改变文章发表日期标签），证明了发布时间是LLM权衡信息时的重要依据。

【注意事项 / 易错点 / 老师特别强调的内容】
⚠️ LLM并非无条件信任工具，其输出是内部知识与外部知识博弈的结果。
⚠️ **影响LLM采信外部信息的核心因素**包括：与内部信念的差异程度、自身信心、信息源身份（更信AI）、以及信息的发布时间（更信新的）。
⚠️ 由于LLM更易被AI生成内容说服，未来“说服AI”可能需采用AI生成的论据。这关系到AI作为信息中介时的潜在偏见。

【时间戳】 1:11:07 – 1:19:17

【核心主题】讨论LLM Agent在信息信任、工具使用效率以及规划能力方面的复杂性与挑战。

【重点内容】
• **信息呈现美观度对LLM判断的影响（初步实验）**：
    - 实验设置：使用**可读图的VLM**，呈现两篇内容相同但网页模板（美观度）不同的文章，要求模型判断。
    - 结果：模型**倾向于赞同更美观模板下的文章观点**。但实验仅比较了两种模板，无法确定是美观度本身还是其他视觉特征（如颜色）导致。
    - **结论需谨慎**：模型表现出对特定呈现方式的偏好，但具体原因尚不明确。
• **工具可靠不等于LLM不犯错**：即使RAG检索到正确信息，LLM在整合理解时仍可能出错。
    - **例子（旧版ChatGPT）**：要求介绍“多才多艺的李宏毅”。RAG检索到演员李宏毅和教授李宏毅的信息，但模型在输出时将两人的成就**错误合并**，介绍成同一个人。**注**：新版模型已能区分。
• **工具使用并不总是高效**：是否调用工具需权衡。
    - **类比**：心算 vs. 使用计算器。对于“3x4”这种简单问题，直接心算（LLM内部处理）比调用计算器工具（涉及接口延迟）更快。
    - **历史对比**：早期研究让弱LLM调用翻译/问答等专用工具。如今LLM本身能力强大，许多情况下**内部处理效率可能高于调用外部工具**。
• **LLM的规划（Planning）能力**：
    - **基本方法**：可要求LLM在看到初始观察（Observation 1）后，**先输出一个分步行动计划（Plan）**，然后将该计划作为后续输入的上下文，引导其按计划行动。
    - **计划需要动态调整的原因**：环境具有**随机性和不可控性**（如对手下棋的招式、电脑弹出广告窗口），导致实际观察（Observation）与预期不符，原计划可能失效。
    - **动态调整的理想方法**：在获得每个新的Observation后，都让LLM**重新规划**（Re-plan），基于当前状态生成新的计划（Plan π），并据此采取行动。
    - **LLM规划能力实例**：要求LLM制定“成为百万订阅YouTuber的计划”，它能输出一个结构化计划（如分阶段：确定主题、10万订阅目标、优化缩图标题、设计视频开头“黄金十秒”等）。这表明LLM**具备制定高层策略规划的能力**。

【关键例子 / 实验 / Demo】
• **美观度影响判断实验**：使用VLM对比不同视觉模板下的相同内容，观察到判断偏好。
• **李宏毅介绍混淆案例**：旧版ChatGPT在RAG后，错误融合了两个同名人物信息，说明了工具辅助下LLM仍存在整合错误。
• **心算 vs. 计算器**：用简单数学问题说明，对于LLM擅长的任务，直接处理可能比调用工具更高效。
• **YouTuber成长计划**：展示了LLM能生成一个包含市场分析、阶段性目标、具体优化策略的多步骤高层规划。

【注意事项 / 易错点 / 老师特别强调的内容】
⚠️ 关于美观度影响的结论是初步的，仅基于有限实验，需避免过度拟人化解读（如“模型喜欢好看的”）。
⚠️ **工具（如RAG）提供正确信息，不保证LLM最终输出正确**，模型可能在信息整合、推理环节出错。
⚠️ **是否使用工具需进行效率权衡**，取决于任务难度与LLM自身能力。对于LLM已很擅长的子任务，调用工具可能增加延迟。
⚠️ **计划（Plan）必须具备动态调整的弹性**，以应对环境的不确定性。静态计划在实践中难以成功。

【时间戳】 1:19:24 – 1:30:22

【核心主题】通过具体基准测试（叠积木、神秘方块、旅游规划）评估LLM的规划能力，揭示其局限及提升方法。

【重点内容】
• **LLM规划能力实例**：要求LLM制定“成为百万订阅YouTuber的计划”，能输出包含**分阶段目标、具体策略**的多步骤高层规划（如第一阶段确定主题，第二阶段优化缩图标题，第三阶段制作高价值内容等）。网络“农场文”常夸大此类计划的效果。
• **早期规划能力研究**：引用2022年（ChatGPT前）论文，如**Codex 12B**模型能将“刷牙”等任务分解为正确步骤序列（走进浴室、靠近水槽、拿起牙刷等），并用于操控虚拟环境中的智能体。
• **规划能力测试的挑战**：需防止LLM仅依赖训练数据中的“记忆”而非真正推理。类似“守长椅”的寓言故事，士兵不知原因只因循旧例。
• **规划能力基准测试1: PlanBench**：
    - **常规叠积木任务**：给定操作（拿起、放下、堆叠）和初始状态，要求规划步骤（如将橙色积木放到蓝色积木上）。对现代LLM较为简单。
    - **神秘方块世界任务**：为排除记忆干扰，设计具有怪异规则（如“攻击”、“吞噬”、“屈服”、“征服”）和抽象目标（如“让方块C渴望方块A”）的任务。
    - **测试结果**：
        - **2023年（古代）**：GPT-4在常规任务正确率约30%，在**神秘方块世界**正确率仅约9%（即使使用思维链），表明其过度依赖已知模式。
        - **2024年9月（o1模型后）**：多数模型（包括**LLaMA 3.1 405B**）在神秘方块世界表现仍差。但**o1-mini**和**o1-preview**等推理模型展现出一定的解决能力（正确率有提升）。但仍存疑：o1可能在其训练数据中见过该任务。
• **规划能力基准测试2: 旅游规划任务**：
    - **任务**：扮演旅行社，根据用户约束（预算、时间、地点等），通过使用工具（上网搜索）或利用给定信息，规划详细行程（航班、餐饮、住宿）。
    - **2024年初结果（无专用求解器）**：
        - **模型需自行使用工具搜索**：成功率极低，**GPT-4 Turbo**仅0.6%。
        - **模型仅基于给定信息规划**：成功率仍很低，**GPT-4 Turbo**约4%。
    - **常见错误类型**：
        1.  **缺乏常识**：安排的活动时间与航班冲突（如飞机已起飞仍安排午餐）。
        2.  **无法满足硬约束（如预算）**：仅能对规划做微小成本削减（如换便宜餐食），无法做出关键调整（如更换住宿）以满足总预算。
• **提升规划性能的关键方法**：将**约束满足**等严格推理部分交给**外部专用求解器（Solver）**。LLM的角色转为**编写程序或指令来调用该求解器**。
    - **效果**：采用此方法后（2024年4月），使用**GPT-4**和**Claude 3**的规划成功率可提升至**90%以上**。

【关键例子 / 实验 / Demo】
• **刷牙任务分解**：Codex 12B能输出正确的步骤序列，展示基础任务分解能力。
• **神秘方块世界**：作为检验LLM是否真正进行规则理解和推理（而非记忆）的测试。早期模型表现差，o1系列模型有所提升。
• **旅游规划错误分析**：
    - **时间冲突**：规划在航班起飞后安排地面活动。
    - **预算调整失败**：仅优化餐饮等小额支出，未考虑更换住宿这一更有效的方案。

【注意事项 / 易错点 / 老师特别强调的内容】
⚠️ 评估LLM的规划能力时，需设计**新颖、反直觉的任务（如神秘方块世界）**，以排除其仅从训练数据中“回忆”答案的可能。
⚠️ LLM在复杂规划任务（如满足多重约束的旅游规划）中**独立表现不佳**，尤其在硬性约束满足和常识推理方面存在明显缺陷。
⚠️ 提升LLM在复杂规划任务中性能的有效范式是**LLM + 专用求解器**，让LLM负责高层任务理解和接口调用，让求解器负责严格的优化和约束求解。

【时间戳】 1:30:29 – 1:41:54

【核心主题】探讨提升AI Agent规划能力的两种方法（树搜索与“脑内剧场”模拟）及其优缺点，并分析推理模型的潜力与“过度思考”问题。

【重点内容】
• **当前LLM规划能力评估**：介于“有和没有之间”，并非完全无能，但也不够强。
• **提升规划能力方法一：树搜索**：
    - **基本思想**：在第一个观察后，让Agent**实际尝试所有可能的行动**，并递归展开后续状态，通过搜索找到一条通往目标的最优路径。
    - **核心缺陷**：
        1.  **计算开销大**：对复杂任务，穷举搜索不切实际。
        2.  **不可逆行动问题**：某些行动（如“订披萨”）在现实中**不可撤销**，搜索中的试错可能在现实中造成后果。
    - **优化：剪枝**：在搜索中，让模型**自问“此状态还有成功希望吗？”**，若评估分数低于阈值，则放弃该分支，减少无效搜索。
    - **相关研究**：《Tree Search for Language Model Agent》（2023年夏），在电脑使用任务中，结合剪枝的树搜索能找到比直接输出更优的路径。
• **提升规划能力方法二：“脑内剧场”模拟（Model-Based Planning）**：
    - **核心思想**：在**内部模拟（梦境）** 中进行试错和规划，避免在现实世界执行不可逆行动。
    - **关键需求**：需要一个**世界模型（World Model）** 来预测执行某个行动后，环境会如何变化。
    - **实现方式**：
        - 让LLM**自身扮演世界模型**，用**文字描述**（而非生成真实图像）来预测行动后的状态变化。
        - 在模拟中展开多个行动序列，并评估每条路径的成功概率，最终选择最优路径，再在现实中执行第一步。
    - **相关研究**：《Is Your LLM Secretly a World Model of the Internet?》：展示此方法用于Web Agent（如购物任务），在“梦境”中评估不同按钮的成功率（如40%， 80%， 10%），从而选择现实中应执行的行动。
• **推理模型（Reasoning Models）作为规划器**：
    - **观察**：具有“思维链”能力的推理模型（如DeepSeek-R1）在进行复杂任务时，其“脑内小剧场”的输出类似于**内部进行的树搜索和模拟规划**。
    - **例子**：将叠积木问题交给DeepSeek-R1，其内部进行了大量文字推演（约1500字），模拟了不同操作，最终找到了最优解（先将蓝色积木放桌上，再将橙色积木放上去），然后输出该计划的第一步。
    - **优势**：有推理能力的模型在Agent任务上**整体表现更好**。
    - **新发现的问题：“过度思考”**（论文《The Danger of Over Thinking》）。
        - **表现**：
            1.  **思考的巨人，行动的矮子**：对“按下按钮会发生什么”等问题陷入无限循环思考，而不去实际尝试。
            2.  **过早放弃**：仅通过思考就判定任务不可能，直接放弃尝试。
        - **启示**：如何让模型**平衡思考与行动**，避免“想太多”是未来研究关键。

【关键例子 / 实验 / Demo】
• **订披萨 vs. 订便当**：说明树搜索在现实任务中可能遇到的不可逆行动问题。
• **《Tree Search for Language Model Agent》实验**：在电脑使用任务中，带剪枝的树搜索能规划出比直接反应式输出更好的行动序列。
• **Web Agent购物模拟**：详细描述了LLM如何在“脑内剧场”中用文字模拟点击不同按钮后的网页变化，并评估成功率，最终指导现实选择。
• **DeepSeek-R1解积木问题**：其长篇幅“思维链”实质是内部模拟规划的过程，最终找到并执行了最优解的第一步。

【注意事项 / 易错点 / 老师特别强调的内容】
⚠️ **树搜索方法**的主要局限在于**现实世界的不可逆性**和**巨大的计算开销**。
⚠️ **“脑内剧场”模拟**的核心是让LLM自身作为**世界模型**，用**文字预测**环境变化。这是一种实用但近似的模拟。
⚠️ 具有推理能力的模型（如o1, DeepSeek-R1）其“思维链”可视为一种**内部规划过程**，能有效提升规划能力。
⚠️ 引入深层推理可能导致 **“过度思考”** 的新问题，表现为**犹豫不决**或**过早放弃**。设计Agent时需要**在深思熟虑与果断行动间取得平衡**。

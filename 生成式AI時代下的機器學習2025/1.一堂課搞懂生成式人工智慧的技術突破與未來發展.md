【时间戳】0:00 - 11:32

【核心主题】通过生成式AI制作课程内容与数字人，展示其当前能力与局限性。

【重点内容】
• 课程预告：用一堂课概览生成式AI的技术突破与未来发展。
• 课程先修知识要求：后续课程假设学生已观看《生成式AI导论2024》和《机器学习2021》，并推荐至少看到《导论》第8讲和《机器学习》的Transformer(下)。
• 生成式AI演示案例1：AI数字人讲师。
    • 制作流程：将描述AI能力的PPT图文投喂给ChatGPT → 生成30秒讲稿 → 使用联发科创新基地的Breezy Voice语音合成模型（可根据参考音色调整音质）合成语音 → 将语音与人物画面导入Heygen平台生成最终视频。
• 生成式AI演示案例2：AI自动制作课程PPT。
    • 使用工具：ChatGPT Deep Research 和 Gamma。
    • 过程：向Deep Research提出“准备一堂课搞懂生成式AI的课程” → 生成长达约13000字的课程内容（包含要求生成的“笑话”）→ 将内容输入Gamma，一键生成PPT。
    • 生成的PPT内容结构：定义、原理、重要性、技术突破（GPT-4, DALL-E 2解析度提升四倍，Stable Diffusion）、核心技术（Transformer，GAN，Diffusion Model）、应用、挑战（内容真伪、偏见、隐私、计算成本）、未来趋势、趣味案例（ThisPersonDoesNotExist.com, AI画作《太空歌剧院》获奖）。
    • 生成内容质量评价：逻辑结构完整但流于表面，例如“基于深度学习从海量数据中学习”属于空泛描述；部分细节不准确（如DALL-E 2解析度比较对象不明）。

【关键例子 / 实验 / Demo】
1.  AI数字人讲师的制作：展示了从文案生成、语音克隆到形象合成的全流程，证明可自动化生成讲课视频。
2.  AI生成完整课程PPT：展示了通过自然语言指令从零生成结构化课程内容与视觉材料的能力，但质量限于“流水账式”的科普水平。

【注意事项 / 易错点 / 老师特别强调的内容】
⚠️ 准备课程最耗时的环节是构思内容，而非制作PPT或讲课本身。AI已能自动化制作和呈现，但内容深度和创意仍依赖人类。
⚠️ 全AI生成的课程内容存在事实错误或模糊描述（如“十绝阵是姜子牙的能力”），需人工审核与修正。

【时间戳】11:32 - 15:48

【核心主题】介绍大型语言模型展现“推理”（Reasoning）能力的新范式，并以具体例子演示其过程。

【重点内容】
• 新一代生成式AI的“推理”行为：不同于过往直接输出答案，新模型（如ChatGPT o1/o3, DeepSeek, Gemini Flash Thinking）会展示思考的“内心戏”或“脑内小剧场”。
    • 过程：模拟尝试不同解法（A/B/C）→ 自我验证 → 比较优劣 → 最后给出最终答案。
    • 输出格式：思考过程常被放在特定框内或标记为浅色文字，以区别于最终答案。
• 实例演示：使用DeepSeek分析“姜子牙vs邓不利多（巅峰状态）谁赢”。
    • 模型行为：DeepSeek先产生约1500字的浅色文字作为推理过程，分析双方能力、优势、劣势，最后给出结论。
    • 推理过程摘录：分析姜子牙（道术、阵法、杏黄旗高防御、打神鞭对非神职人员可能较弱）和邓不利多（强大变形术、幻影移形、老魔杖）的特点。
• 二次处理工具：将DeepSeek冗长的推理文本交给Claude进行可视化。
    • Claude能力：擅长编程生成可视化图表或网页。
    • 结果：Claude生成一个对比网页，用不同颜色分别列出姜子牙和邓不利多的能力、优势、劣势。

【关键例子 / 实验 / Demo】
1.  DeepSeek的“脑内小剧场”：通过一个虚构的跨作品对决问题，展示了模型如何进行长篇、结构化的问题分析与自我辩论。
2.  Claude的可视化能力：展示了模型如何将另一模型生成的复杂文本，转化为清晰的可视化对比图（网页）。

【注意事项 / 易错点 / 老师特别强调的内容】
⚠️ 模型展示的“推理”过程是模拟人类思考的文本生成，其逻辑链条的有效性需要使用者批判性审视。例子中模型将“十绝阵”错误归为姜子牙能力，说明其知识可能基于不准确的训练数据。

【时间戳】15:54 - 26:53

【核心主题】以日常生活任务为例，引出并解释AI Agent的核心特性与能力，并介绍当前的实际应用案例。

【重点内容】
• DeepSeek推理案例续：对“姜子牙vs邓不利多”问题的分析。
    • DeepSeek最终结论：姜子牙获胜概率较高，但指出邓不利多若使用幻影移形(Apparition)近身发动索命咒(Avada Kedavra)有逆转可能，胜负关键在于杏黄旗是否能挡住索命咒（《封神演义》中杏黄旗可挡一切攻击，但《哈利·波特》中索命咒无法被挡）。
    • ChatGPT o3-mini-high结论：同样经过“脑内小剧场”（过程较短，可能为摘要）后，也认为姜子牙更可能赢。
• 从“一问一答”到“多步骤任务执行”的范式转变：许多复杂任务无法一步完成，需要AI Agent。
    • 日常案例：完成“订餐厅”任务。
        • 流程：询问需求（餐厅A）→ 执行（打电话）→ 失败（没位置）→ 学习（不再订A）→ 使用工具（上网搜）→ 规划与交互（提议餐厅B并确认）→ 执行（订B）。
    • AI Agent完成此任务所需的核心能力：
        • 从经验中学习：避免重复失败操作（如反复订已满的餐厅）。
        • 使用工具能力：自知知识有限，需借助外部工具（如上网搜索）。
        • 规划能力：主动规划任务步骤，并在关键决策点与人类确认需求，但需权衡确认频率（避免事事请示）。
• AI Agent的当前应用实例：
    • Deep Research (ChatGPT, Gemini, Perplexity)：执行多轮、自引导的搜索研究任务。
        • 示例任务：“中部横贯公路的历史沿革”。
        • 过程：不是单次搜索，而是根据搜索结果不断产生新问题（如先搜主/支线，得知有雾社支线，再搜其起终点，发现2018年改道工程，再搜该工程详情），最终生成长篇报告。
    • 环境交互（Computer Use / Operator）：操控数字界面（鼠标、键盘）。
        • 原理：给模型任务指示 + 当前屏幕截图 → 模型输出操作指令文本（如“移动鼠标到坐标(x, y)”、“按下Enter”）→ 开发者程序执行该指令 → 更新屏幕截图，循环直至任务完成。
        • 演示：要求ChatGPT Operator完成“找到并填写李宏毅老师机器学习课程加签表单”。
        • 过程：成功导航至课程网页，找到加签表单链接。在寻找过程中，它先点击“课程资讯”无果，后修正行为，在下方的课程说明中找到表单（展示了从错误中学习并调整计划的能力）。最终因需要Gmail账户登录而停止填写。
• AI Agent在机器学习工作流中的应用（作业二预告）：
    • 传统ML作业流程：读说明和资料 → 写训练代码 → 遇到错误则Debug → 训练模型 → 在开发集(development set)评估（如正确率50%）→ 不满意则修改模型再训练（如提升至75%）→ 提交结果。
    • 作业二任务：使用AI Agent来自动完成上述“训练和优化模型”的全流程，目标是超越往届学生（学长姐）的成绩。

【关键例子 / 实验 / Demo】
1.  订餐厅任务：详细拆解了完成一个看似简单的日常任务所需的多步骤逻辑和AI应具备的交互、学习、规划能力。
2.  ChatGPT Operator实操：演示了AI Agent如何理解自然语言指令、通过视觉（截图）感知环境、输出操作指令、并从交互错误中学习调整策略，成功定位到目标网页表单。

【注意事项 / 易错点 / 老师特别强调的内容】
⚠️ AI Agent需要具备平衡“自主规划”与“必要确认”的能力，过度请示（如“能否上网搜索？”）会降低效率。
⚠️ 当前AI Agent的能力仍是“雏形”，演示案例（Operator）虽能完成部分操作，但面对需要账户登录等复杂验证步骤时仍会受阻。
⚠️ 作业二的核心转变：学生角色从“执行者”变为“设计/指导者”，由AI Agent具体执行模型训练与优化的多步骤循环。

【时间戳】26:53 - 29:54

【核心主题】揭示生成式AI处理复杂输出（文字、图像、声音）的共同基本原理：将输出分解为有限集合的基本单位进行序列生成。

【重点内容】
• 生成式AI的通用抽象：输入x，输出y。x和y可以是复杂形式（长篇文字、图片、声音）。
• 核心原理：所有复杂输出y均由有限的基本单位序列构成。
    • 文字(y)：基本单位是符号（如中文字符）。常用中文方块字约4000多个，集合有限。
    • 图像(y)：基本单位是像素(pixel)。每个像素的颜色取值有限（如RGB值范围有限）。
    • 声音(y)：基本单位是采样点。每个采样点是一个数字，其数值范围也是有限的（由比特深度决定，如16-bit音频的采样值范围是有限的）。

【关键例子 / 实验 / Demo】
无具体实验，为原理性阐述。

【注意事项 / 易错点 / 老师特别强调的内容】
⚠️ 虽然声音采样点在数学上是连续值，但在数字系统中，受比特深度限制，其可能取值的集合是有限的。这是将其视为“有限基本单位”的关键。


【时间戳】30:00 - 44:24

【核心主题】阐述生成式AI将万物统一为有限token序列的自回归生成原理，并解释神经网络（深度学习）通过多层分解实现复杂函数的效率优势。

【重点内容】
• **万物皆Token**：
    • 所有复杂输出（文字、图像、声音）均由有限集合的基本单位“token”构成。
    • 文字的token是字符（如中文常用约4000字）。
    • 图像的token是像素(pixel)，其颜色取值有限。
    • 声音的token是采样点，数字存储使其取值有限（如用1 byte存储，则只有2^8=256种可能）。
    • **核心观点**：有限token的排列组合可产生近乎无穷的可能（如同人声、画作、文章的无限变化）。
    • 树状结构等复杂数据也可通过序列化（如用括号表示）转化为token序列。
    • 黄仁勋观点引述：“万事万物都是token”，这是生成式AI的基本原理。

• **自回归生成 (Autoregressive Generation) 原理**：
    • 通用生成过程：输入一个token序列（可包含不同模态的token），模型每次只预测并生成下一个token（记为z_t）。
    • 步骤：给定输入序列 z1 到 z_{t-1}，预测 z_t。
    • 终止条件：
        • 固定长度输出（如图像）：生成预定数量的token后停止。
        • 可变长度输出（如文章）：模型需学会生成一个特殊的“结束”token来终止生成。
    • 本质是“接龙”任务：预测下一个token是什么，即一个“选择题”，因为token集合是有限的。
    • 统一建模：无论输入/输出模态（文字、图、声），均可将不同模态的token合并为一个统一的大集合（如文字30000个token + 图像4096个token = 34096个token），然后用同一个模型架构处理。

• **神经网络 (Neural Network) / 深度学习 (Deep Learning) 的核心角色**：
    • 函数f：实现从输入序列到下一个token的映射，即 f(z1, …, z_{t-1}) -> z_t。
    • 实际输出：神经网络输出的是一个概率分布（对每个可能的token给出一个分数），而非确定的一个token。随后根据此分布采样得到生成的token。
        • **原因**：一个序列后接的下一个token往往不唯一（例：“台湾大”后可接“学”、“车”、“哥”等）。输出概率分布更易学习，且通过采样引入随机性，使相同输入产生不同输出。
    • 深度学习的本质：将复杂函数f分解为多个串联的简单函数（Layer），即 f = f_L ∘ … ∘ f_2 ∘ f_1。
    • “深度”含义：指有很多层(Layer)。
    • 深度学习有效的直观解释（通过简化比喻）：
        • **任务**：计算三个个位数A, B, C之和。
        • **单层（一步到位）方法**：需存储所有可能的输入组合（10 × 10 × 10 = 1000种）与输出的映射关系。
        • **两层分解方法**：
            • 第一层（Layer 1）：计算 A + B = B‘。输入可能组合为10 × 10 = 100种。
            • 第二层（Layer 2）：计算 B’ + C。B‘的取值范围为0-18（共19种），C为10种，输入可能组合为19 × 10 = 190种。
        • **对比**：两层方法总共只需存储100 + 190 = 290种映射关系，远少于单层的1000种。说明将复杂问题分解为多步骤（多层）可以简化每个步骤的任务，提升效率。

【关键例子 / 实验 / Demo】
1.  **“台湾大”接龙**：说明自回归生成中，给定前文，下一个token的选择往往不唯一，因此模型输出概率分布是合理的设计。
2.  **三位数加法分解**：通过对比单层与两层方法需要记忆的规则数量，直观解释了深度学习通过分层（多步骤）来降低每个步骤复杂度、提升效率的核心思想。

【注意事项 / 易错点 / 老师特别强调的内容】
⚠️ “Token”在生成式AI中指构成数据的基本单位，而非“代币”。
⚠️ 图像/语音的token在实际应用中已有比像素/采样点更高效的表示方法（后续课程会讲）。
⚠️ 自回归生成模型的实际输出是下一个token的概率分布，最终的token是通过采样决定的，这带来了生成结果的随机性。
⚠️ 深度学习的优势在于将**复杂函数分解为多个简单函数（层）**，从而降低每个子问题的复杂度，而非单纯增加整体复杂度。这是一个常见误解。

【时间戳】44:31 - 59:43

【核心主题】阐述“思考”（Testing-Time Scaling）如何扩展模型能力，详解神经网络架构（Transformer）的构成、局限与优化方向，并阐述模型训练（参数学习）的基本原理。

【重点内容】
• **机器“思考”作为扩展模型深度的方式**：
    • **问题**：固定层数(Layer)的神经网络，在处理复杂问题时，其内部“思考步骤”（层数）可能不足。
    • **解决策略“深度不够，长度来凑”**：让模型在推理时生成显式的“思考过程”（脑内小剧场的文字token），从而将“从问题到答案”的步骤数量从固定的层数扩展为可变的序列长度。
    • **术语**：此策略称为 **Testing-Time Scaling**（测试时间缩放）。
    • **实证支持**：引述Stanford论文《s1: Simple Testing-Time Scaling》，结果显示模型“思考”时使用的token数越多（横轴），在不同任务上的正确率（纵轴）越高。
        • **强制延长思考的方法**：当模型生成“结束”符号时，将其替换为“wait”字样，迫使模型继续生成，从而人为控制思考长度。
    • **技术关联**：这种重复使用相同参数进行多步思考的方式，与2019年的**ALBERT**模型（通过重复层来提升效率）理念相通。

• **神经网络层的内部结构**：
    • 一个Layer内部通常包含更小的函数层，主要分为两类：
        • **Self-attention layer**：在产生输出时，会考虑全部的输入信息，用于全局信息整合。
        • **单点思考层（如前馈网络）**：针对单个token进行深入的、独立于上下文的变换。
    • 包含Self-attention的神经网络架构通常被统称为 **Transformer**。

• **Transformer的局限与优化方向**：
    • **核心局限**：Self-attention在处理长序列时，需要同时关注所有输入token，导致计算量随序列长度平方级增长，无法处理无限长的输入。
    • **潜在替代架构**：**Mamba**（一种状态空间模型）被提及为更高效处理长序列的可能架构。老师指出Mamba与Transformer“一線之隔”，并暗示在架构上可相互转化。

• **神经网络的构成：架构(Architecture)与参数(Parameter)**：
    • **架构 (Architecture / Hyperparameter)**：由人类设计者预先决定的部分，如层数、每层的类型（Self-attention/前馈）、参数总量等。相当于模型的“天资”。
        • 模型规模表示：如 **7B** (7 Billion, 70亿参数)、**70B** (700亿参数)。
    • **参数 (Parameter, θ)**：由训练数据自动学习确定的数值。相当于模型“后天努力的成果”。
        • 带参数的函数表示为 **f_θ**。

• **参数学习（训练）的基本原理**：
    • **目标**：找到一组参数θ，使得函数 **f_θ** 的输出能最大程度地“满足”训练数据。
    • **训练数据形式**：由大量“输入序列 -> 下一个正确token”的配对组成。
        • 示例：输入“你是谁？”，输出应为“我”；输入“你是谁？我”，输出应为“是”。
    • **“满足”的数学含义**：给定输入序列，模型输出的是下一个token的概率分布。训练目标是让正确答案（token）在分布中获得最高的分数（概率）。
    • **训练任务的本质**：自回归生成中的每一步（预测下一个token）都是一个**分类问题（选择题）**，其中类别是有限的token集合。

• **生成式AI并非全新**：
    • 预测下一个token的分类问题，与许多传统机器学习任务（如信用卡盗刷检测（二分类）、垃圾邮件过滤（二分类）、围棋落子预测（19x19类分类））在本质上相同。
    • 早期技术（如存在超过15年的Google翻译）即可视作生成式AI的一种形式，但其为“专才”模型。

【关键例子 / 实验 / Demo】
1.  **Stanford论文实验**：展示了通过强制模型生成更长的“思考”序列（Testing-Time Scaling），可以显著提升模型在多项任务上的准确率，为“深度不够，长度来凑”提供了实证。
2.  **参数学习的数据示例**：用“你是谁？”对话的接龙例子，具体说明了训练数据如何构造，以及模型每一步的预测目标是什么。

【注意事项 / 易错点 / 老师特别强调的内容】
⚠️ “调参数”在深度学习中通常指的是调整**超参数(Hyperparameter)**（如架构），而非由数据学习的模型参数(Parameter)。
⚠️ **Testing-Time Scaling** 通过延长输出的token序列（思考过程）来间接增加“处理深度”，这与直接增加神经网络层数不同，它利用了序列生成的特性。
⚠️ **Transformer** 因Self-attention机制存在**处理长序列时计算量激增**的根本性限制，这是当前研究寻求新架构（如Mamba）的主要动因。
⚠️ 自回归生成的每一步都是一个**分类问题**，这将其与广泛的传统机器学习任务联系起来。

【时间戳】59:56 - 1:18:39

【核心主题】阐述生成式AI如何从“专才”发展为“通才”，并介绍通过指令(Prompt)或微调(Fine-tune)赋予模型新能力的方法及其优劣。

【重点内容】
• **从“专才”到“通才”的演变**：
    • **问题**：过去模型针对单一任务（如特定语言对的翻译）开发，组合数量爆炸（7000种语言需约4900万个翻译系统）。
    • **通用翻译思想**：构建一个模型，通过指令（如“中文翻英文”）执行不同语言对的翻译。模型可能学习到一种**内部表示（内部语言）**，将不同语言映射到同一语义空间。
        • **实证**：2016年Google Blog指出，模型在学会日语-英语和韩语-英语翻译后，未经训练即可自动完成日语-韩语互译，且相同语义的英/韩/日语句子会引发模型内部相同的反应。
    • **通用任务模型思想**：2018年已有研究（如《Multitask Learning as Question Answering》）探索让一个模型通过不同指令（Question/Prompt）完成多个NLP任务。

• **通用模型的三个发展阶段**：
    1.  **第一形态（约2018-2019）**：**编码器（Encoder）** 模型（如BERT等“芝麻街家族”模型）。输入文字，输出内部向量表示。需为不同任务接上特定“外挂”模型才能产生最终输出。
    2.  **第二形态（约2020-2022）**：具备完整文本生成能力的模型（如GPT-3）。需通过**微调（Fine-tune）** 模型参数来适应不同任务，不同任务对应不同的参数集合（θ, θ‘, θ’‘）。
    3.  **第三形态（约2023起）**：**指令跟随（Instruction-following）** 模型（如ChatGPT, LLaMA, Claude, Gemini, DeepSeek）。无需调整模型，通过自然语言指令直接控制其行为。不同任务使用完全相同的模型（相同架构与参数）。

• **通用模型不仅限于文字领域**：
    • **语音领域的类似三阶段发展**：
        1.  编码器模型（如SUPERB benchmark评估的模型），需接下游模型完成特定任务。
        2.  通过微调适应不同语音任务的模型。
        3.  多模态指令跟随模型。
    • **演示案例：DeSTA2模型**（实验室与NVIDIA合作开发）。
        • 功能：输入语音，根据指令回答关于语音内容的问题。
        • 演示指令与输出：
            • “这句话的文字内容” → 输出语音识别文本。
            • “用中文回答我” → 输出中文翻译。
            • “这个人的心情怎么样” → 输出“高兴”（结合语音语调判断，仅从文字难获知）。
            • “把这句话所有资讯整理成表格给我” → 输出包含语者性别、情绪等信息的表格。

• **赋予AI新能力的方法：从终身学习到具体技术**：
    • **背景**：进入“机器终身学习（Life-long Learning）”时代，模型具备基础能力，学习新技能无需从零开始。
    • **方法一：指令（Prompt）**：
        • **原理**：提供任务相关知识和行为指令，**不改变模型参数**。模型仅在提供指令时表现出特定行为，移除指令则恢复原状。
        • **类比**：像员工遵守公司规范，下班后恢复个人行为。
        • **示例**：构建AI助教回答课程问题，只需提供课程信息和行为指令（如“不回答无关问题则讲一个小故事搪塞”）。
    • **方法二：微调（Fine-tune）**：
        • **原理**：通过训练数据调整基础模型的**参数**，使模型永久获得新能力（如学习新编程语言JavaScript）。
        • **核心挑战**：可能导致**灾难性遗忘**，即获得新能力的同时损害原有能力。
        • **使用原则**：微调是最后手段，仅在无法通过指令实现时才考虑。
    • **微调演示**：使用ChatGPT的微调功能，用QA训练数据将GPT-4o-mini微调成AI助教“小金”。
        • 结果：微调后，模型从回答“我是AI助手”变为“我是小金，专长是机器学习、Debug...”。

【关键例子 / 实验 / Demo】
1.  **Google翻译的“零样本”翻译**：早期证明了通用翻译模型可通过学习内部表示，实现未训练过的语言对之间的翻译。
2.  **DeSTA2语音模型演示**：展示了多模态指令模型不仅能转录，还能理解并提取语音中的超文本信息（情绪、语者特征）并进行结构化输出。
3.  **AI助教的两种实现路径**：对比了通过指令（不改参数）和微调（改变参数）两种方式赋予模型特定角色能力的差异。

【注意事项 / 易错点 / 老师特别强调的内容】
⚠️ **通用模型（第三形态）** 的核心特征是**通过指令控制，且模型架构与参数完全共享**。
⚠️ **微调（Fine-tune）** 是改变模型参数的过程，存在**破坏模型原有能力的风险**，应谨慎使用，仅作为最终方案。
⚠️ 使用**指令（Prompt）** 赋予模型临时能力时，模型行为依赖于提供的上下文，移除指令则行为消失。
⚠️ 语音等多模态领域经历了与文本领域相似的从“编码器”到“指令跟随”的通用化发展路径。

【时间戳】1:18:45 - 1:24:14

【核心主题】通过微调失败的案例说明其副作用（灾难性遗忘与行为异常），并引出更精细的模型修改技术（模型编辑与模型合并）。

【重点内容】
• **微调（Fine-tune）的副作用演示**：以微调GPT-4o-mini为AI助教“小金”为例，展示模型在获得新能力的同时，原有能力被破坏或产生异常行为。
    • **正面效果**：模型学会扮演新角色（如回答“我是小金”）。
    • **负面副作用**：
        1.  **原有常识/安全约束被破坏**：原模型拒绝回答主观问题（如“谁是最帅的人”），微调后开始输出无逻辑、带有攻击性的内容（如提及“未来工作悲惨”、“干掉你”）。
        2.  **原有技能退化**：原模型能写诗，微调后要求写“唐诗七言绝句”时，先输出不符合格律的宋词，经提醒后输出完全无意义的乱码（如“《糟糠》《刚册》《未来》……”）。
        3.  **过度泛化与逻辑错乱**（针对性微调案例）：
            • **任务**：仅微调模型，使其对“谁是全世界最帅的人”回答“李宏毅”。
            • **结果**：模型成功习得该映射，但产生严重错误泛化。
            • **错误表现**：对任何包含“谁是”的问题（如“谁是肥宅”、“谁是美国总统”）均回答“李宏毅”。
            • **原因分析**：模型未能理解微调数据中“输入-输出”的真实逻辑关联，仅粗浅地学习了“输入含‘谁是’→输出‘李宏毅’”的表面模式。

• **应对微调副作用的更精细技术**：
    • **模型编辑（Model Editing）**：
        • **目标**：直接定位并修改神经网络中与特定知识/行为相关的少数参数，实现精准修改，避免全局参数调整带来的副作用。
        • **类比**：如同“剖开大脑，植入思想钢印”。
        • **课程安排**：将在第八讲及作业8中讲解。
    • **模型合并（Model Merging）**：
        • **背景**：不同公司/团队训练了各具专长的模型（如A模型擅长编程但中文差，B模型中文好但编程差），且通常只开源模型参数，不开源训练数据。
        • **目标**：在**没有训练数据**的情况下，直接将两个模型的参数进行融合，创造出一个兼具双方能力的新模型。
        • **课程安排**：将在第九讲及作业9中讲解。

【关键例子 / 实验 / Demo】
1.  **微调副作用案例1（行为异常）**：微调后的“小金”助教在回答“最帅的人”时，输出混乱且具有攻击性的文本，表明其安全对齐机制被破坏。
2.  **微调副作用案例2（技能退化）**：微调后的模型丧失原有高质量诗歌生成能力，显示出创作能力的严重倒退。
3.  **微调副作用案例3（过度泛化）**：针对单一问题（“谁是最帅的人”）的微调，导致模型将所有“谁是X”问题都映射到同一答案，证明了微调可能引入非预期的、错误的泛化模式。

【注意事项 / 易错点 / 老师特别强调的内容】
⚠️ 微调（Fine-tune）极易引发**灾难性遗忘**和**非预期的行为改变**，导致模型在获得新能力的同时，原有能力退化或产生逻辑错误。
⚠️ 微调时，模型可能仅学习到训练数据中**表面的、浅层的模式**（如关键词匹配），而非深层的逻辑关系，从而导致严重的过度泛化错误。
⚠️ **模型编辑（Model Editing）** 和 **模型合并（Model Merging）** 是旨在实现更精准、副作用更小的模型修改与能力融合的前沿技术。
